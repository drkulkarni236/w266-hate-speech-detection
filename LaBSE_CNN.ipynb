{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab_cnn_korean_266.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PIP INSTALLS & IMPORTS NEEDED"
      ],
      "metadata": {
        "id": "qvq87Eb3QzKZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUGxFvZDE5L8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b155fce-d9ae-45e2-f363-9779b6715126"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.7/dist-packages (0.12.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub) (1.21.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow-hub) (1.15.0)\n",
            "Collecting tensorflow-text\n",
            "  Downloading tensorflow_text-2.8.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow<2.9,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text) (2.8.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text) (0.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (3.10.0.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (3.3.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (2.8.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (2.8.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (2.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (13.0.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (3.17.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (57.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (0.24.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.0.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.21.5)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.1.2)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (0.5.3)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 57.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.6.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.14.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.44.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (0.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<2.9,>=2.8.0->tensorflow-text) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.5.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (3.3.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (3.2.0)\n",
            "Installing collected packages: tf-estimator-nightly, tensorflow-text\n",
            "Successfully installed tensorflow-text-2.8.1 tf-estimator-nightly-2.8.0.dev2021122109\n",
            "Collecting tf-models-official\n",
            "  Downloading tf_models_official-2.8.0-py2.py3-none-any.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (1.3.5)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (4.1.3)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (2.0.4)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 10.6 MB/s \n",
            "\u001b[?25hCollecting tf-slim>=1.1.0\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "\u001b[K     |████████████████████████████████| 352 kB 70.8 MB/s \n",
            "\u001b[?25hCollecting py-cpuinfo>=3.3.0\n",
            "  Downloading py-cpuinfo-8.0.0.tar.gz (99 kB)\n",
            "\u001b[K     |████████████████████████████████| 99 kB 10.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (0.12.0)\n",
            "Requirement already satisfied: tensorflow-text~=2.8.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (2.8.1)\n",
            "Collecting tensorflow-model-optimization>=0.4.1\n",
            "  Downloading tensorflow_model_optimization-0.7.2-py2.py3-none-any.whl (237 kB)\n",
            "\u001b[K     |████████████████████████████████| 237 kB 49.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (3.2.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (7.1.2)\n",
            "Collecting opencv-python-headless\n",
            "  Downloading opencv_python_headless-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 47.8 MB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (0.29.28)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 43.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (1.15.0)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (5.4.8)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (1.21.5)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (4.0.1)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (1.5.12)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (1.12.11)\n",
            "Collecting pyyaml<6.0,>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 67.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (0.5.0)\n",
            "Requirement already satisfied: tensorflow~=2.8.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official) (2.8.0)\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.16.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 63.1 MB/s \n",
            "\u001b[?25hCollecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.17.4)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.0.4)\n",
            "Requirement already satisfied: google-auth<3dev,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (1.35.0)\n",
            "Requirement already satisfied: google-api-core<3dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (1.26.3)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (3.0.1)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (21.3)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (2.23.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (1.56.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (3.17.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (2018.9)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (57.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official) (4.2.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official) (1.24.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official) (4.63.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official) (2021.10.8)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official) (6.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (3.0.7)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official) (2.10)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (13.0.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (3.1.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (2.8.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (0.5.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (3.10.0.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (1.14.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (1.0.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (0.24.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (3.3.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (1.1.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0->tf-models-official) (1.44.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow~=2.8.0->tf-models-official) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow~=2.8.0->tf-models-official) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official) (0.4.6)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official) (3.7.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow~=2.8.0->tf-models-official) (3.2.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official) (0.1.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->tf-models-official) (1.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->tf-models-official) (0.11.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official) (1.3)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu->tf-models-official) (0.8.9)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu->tf-models-official) (2019.12.20)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval->tf-models-official) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (3.1.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->tf-models-official) (2.7.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official) (0.3.4)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official) (5.4.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official) (2.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official) (0.16.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official) (1.7.0)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official) (21.4.0)\n",
            "Building wheels for collected packages: py-cpuinfo, seqeval\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-cpuinfo: filename=py_cpuinfo-8.0.0-py3-none-any.whl size=22257 sha256=a05ae275017b9e2224f48dd3a38270550d931ea013ad92aa3b15606795cc6155\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/f1/1f/041add21dc9c4220157f1bd2bd6afe1f1a49524c3396b94401\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=c5b83e6e0d86cde3dd4ba9317c7dbdfa68c66a81cf01bfb51d9cd769b416433f\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "Successfully built py-cpuinfo seqeval\n",
            "Installing collected packages: portalocker, colorama, tf-slim, tensorflow-model-optimization, tensorflow-addons, seqeval, sentencepiece, sacrebleu, pyyaml, py-cpuinfo, opencv-python-headless, tf-models-official\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed colorama-0.4.4 opencv-python-headless-4.5.5.64 portalocker-2.4.0 py-cpuinfo-8.0.0 pyyaml-5.4.1 sacrebleu-2.0.0 sentencepiece-0.1.96 seqeval-1.2.2 tensorflow-addons-0.16.1 tensorflow-model-optimization-0.7.2 tf-models-official-2.8.0 tf-slim-1.1.0\n",
            "Collecting bert-for-tf2\n",
            "  Downloading bert-for-tf2-0.14.9.tar.gz (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 176 kB/s \n",
            "\u001b[?25hCollecting py-params>=0.9.6\n",
            "  Downloading py-params-0.10.2.tar.gz (7.4 kB)\n",
            "Collecting params-flow>=0.8.0\n",
            "  Downloading params-flow-0.8.2.tar.gz (22 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.21.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.63.0)\n",
            "Building wheels for collected packages: bert-for-tf2, params-flow, py-params\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.9-py3-none-any.whl size=30535 sha256=1a16c36f68311a81304a680eb6c10b5803aba43b6907455dd1b1287caaf211f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/b6/e5/8c76ec779f54bc5c2f1b57d2200bb9c77616da83873e8acb53\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.8.2-py3-none-any.whl size=19472 sha256=6b5751a7180c06fe357857956aa7509d528517fb662cf92acd62e2cc3e4ca40f\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/fc/d2/a44fff33af0f233d7def6e7de413006d57c10e10ad736fe8f5\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.10.2-py3-none-any.whl size=7911 sha256=ab554bd8cba5c05ea018ee1ac0062ce116b0ae637a3c9f2f17907828628f1a14\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/11/67/33cc51bbee127cb8fb2ba549cd29109b2f22da43ddf9969716\n",
            "Successfully built bert-for-tf2 params-flow py-params\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.14.9 params-flow-0.8.2 py-params-0.10.2\n"
          ]
        }
      ],
      "source": [
        "# Install tensor flow\n",
        "!pip install tensorflow-hub\n",
        "!pip install tensorflow-text # Needed for loading universal-sentence-encoder-cmlm/multilingual-preprocess\n",
        "!pip install tf-models-official\n",
        "!pip install bert-for-tf2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VARIOUS IMPORTS"
      ],
      "metadata": {
        "id": "VvmMIDeeIpHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Various other import statements\n",
        "import bert\n",
        "from official.nlp import optimization\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text \n",
        "\n",
        "import tensorflow.keras as keras\n",
        "from sklearn.metrics import f1_score,precision_score,recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "from keras.layers import Input, Lambda, Dense, Dropout\n",
        "from tensorflow.keras.metrics import Metric\n",
        "\n",
        "# early callback\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n"
      ],
      "metadata": {
        "id": "F1IAL1tzFaHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Additional imports\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "\n"
      ],
      "metadata": {
        "id": "J7P4KT9HF5w_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CUSTOM METRIC METHODS"
      ],
      "metadata": {
        "id": "xEtIDgMgBn2A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A CUSTOM CLASS FOR CALCULATING F1 SCORE"
      ],
      "metadata": {
        "id": "jawq0rQPz9Sb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StatefullBinaryFBeta(Metric):\n",
        "  def __init__(self, name='state_full_binary_fbeta', beta=1, threshold=0.5, epsilon=1e-7, **kwargs):\n",
        "    # initializing an object of the super class\n",
        "    super(StatefullBinaryFBeta, self).__init__(name=name, **kwargs)\n",
        "\n",
        "    # initializing state variables\n",
        "    self.tp = self.add_weight(name='tp', initializer='zeros') # initializing true positives \n",
        "    self.actual_positive = self.add_weight(name='fp', initializer='zeros') # initializing actual positives\n",
        "    self.predicted_positive = self.add_weight(name='fn', initializer='zeros') # initializing predicted positives\n",
        "\n",
        "    # initializing other atrributes that wouldn't be changed for every object of this class\n",
        "    self.beta_squared = beta**2 \n",
        "    self.threshold = threshold\n",
        "    self.epsilon = epsilon\n",
        "\n",
        "  def update_state(self, ytrue, ypred, sample_weight=None):\n",
        "    # casting ytrue and ypred as float dtype\n",
        "    ytrue = tf.cast(ytrue, tf.float32)\n",
        "    ypred = tf.cast(ypred, tf.float32)\n",
        "\n",
        "    # setting values of ypred greater than the set threshold to 1 while those lesser to 0\n",
        "    ypred = tf.cast(tf.greater_equal(ypred, tf.constant(self.threshold)), tf.float32)\n",
        "        \n",
        "    self.tp.assign_add(tf.reduce_sum(ytrue*ypred)) # updating true positives atrribute\n",
        "    self.predicted_positive.assign_add(tf.reduce_sum(ypred)) # updating predicted positive atrribute\n",
        "    self.actual_positive.assign_add(tf.reduce_sum(ytrue)) # updating actual positive atrribute\n",
        "\n",
        "  def result(self):\n",
        "    self.precision = self.tp/(self.predicted_positive+self.epsilon) # calculates precision\n",
        "    self.recall = self.tp/(self.actual_positive+self.epsilon) # calculates recall\n",
        "\n",
        "    # calculating fbeta\n",
        "    self.fb = (1+self.beta_squared)*self.precision*self.recall / (self.beta_squared*self.precision + self.recall + self.epsilon)\n",
        "    \n",
        "    return self.fb\n",
        "\n",
        "  def reset_state(self):\n",
        "    self.tp.assign(0) # resets true positives to zero\n",
        "    self.predicted_positive.assign(0) # resets predicted positives to zero\n",
        "    self.actual_positive.assign(0) # resets actual positives to zero"
      ],
      "metadata": {
        "id": "PJ2np2r_znXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CUSTOM CLASS FOR MACRO F1"
      ],
      "metadata": {
        "id": "lcZiOL1mEI2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StatefullMultiClassFBeta(Metric):\n",
        "  def __init__(self, name='state_full_binary_fbeta_macro', beta=1, n_class=1, average='macro', epsilon=1e-7, **kwargs):\n",
        "    # initializing an object of the super class\n",
        "    super(StatefullMultiClassFBeta, self).__init__(name=name, **kwargs)\n",
        "\n",
        "    # initializing state variables\n",
        "    self.tp = self.add_weight(name='tp', shape=(n_class,), initializer='zeros')     # initializing true positives\n",
        "    self.actual_positives = self.add_weight(name='ap', shape=(n_class,), initializer='zeros') # initializing actual positives\n",
        "    self.predicted_positives = self.add_weight(name='pp', shape=(n_class,), initializer='zeros') # initializing predicted positives\n",
        "\n",
        "    # initializing other atrributes that wouldn't be changed for every object of this class\n",
        "    self.beta_squared = beta**2\n",
        "    self.n_class = n_class\n",
        "    self.average = average\n",
        "    self.epsilon = epsilon\n",
        "\n",
        "  def update_state(self, ytrue, ypred, sample_weight=None):\n",
        "    # casting ytrue and ypred as float dtype\n",
        "    ytrue = tf.cast(ytrue, tf.float32)\n",
        "    ypred = tf.cast(ypred, tf.float32)\n",
        "\n",
        "    # finding the maximum probability in ypred\n",
        "    max_prob = tf.reduce_max(ypred, axis=-1, keepdims=True)\n",
        "\n",
        "    # making ypred one hot encoded such that the class with the maximum probability as encoded as 1 while others as 0\n",
        "    ypred = tf.cast(tf.equal(ypred, max_prob), tf.float32)\n",
        "        \n",
        "    self.tp.assign_add(tf.reduce_sum(ytrue*ypred, axis=0)) # updating true positives atrribute\n",
        "    self.predicted_positives.assign_add(tf.reduce_sum(ypred, axis=0)) # updating predicted positives atrribute\n",
        "    self.actual_positives.assign_add(tf.reduce_sum(ytrue, axis=0)) # updating actual positives atrribute\n",
        "\n",
        "  def result(self):\n",
        "    self.precision = self.tp/(self.predicted_positives+self.epsilon) # calculates precision\n",
        "    self.recall = self.tp/(self.actual_positives+self.epsilon) # calculates recall\n",
        "\n",
        "    # calculating fbeta score\n",
        "    self.fb = (1+self.beta_squared)*self.precision*self.recall / (self.beta_squared*self.precision + self.recall + self.epsilon)\n",
        "\n",
        "    if self.average == 'weighted':\n",
        "      return tf.reduce_sum(self.fb*self.actual_positives / tf.reduce_sum(self.actual_positives))\n",
        "    \n",
        "    elif self.average == 'raw':\n",
        "      return self.fb\n",
        "\n",
        "    return tf.reduce_mean(self.fb)\n",
        "\n",
        "  def reset_state(self):\n",
        "    self.tp.assign(tf.zeros(self.n_class)) # resets true positives to zero\n",
        "    self.predicted_positives.assign(tf.zeros(self.n_class)) # resets predicted positives to zero\n",
        "    self.actual_positives.assign(tf.zeros(self.n_class)) # resets actual positives to zero"
      ],
      "metadata": {
        "id": "VUTwdmNVwAaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CUSTOM CLASS FOR WEIGHTED F1"
      ],
      "metadata": {
        "id": "WoajsKFyVXFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StatefullMultiClassFBetaWeighted(Metric):   \n",
        "  def __init__(self, name='state_full_binary_fbeta_weighted', beta=1, n_class=1, average='weighted', epsilon=1e-7, **kwargs):\n",
        "    # initializing an object of the super class\n",
        "    super(StatefullMultiClassFBetaWeighted, self).__init__(name=name, **kwargs)\n",
        "\n",
        "    # initializing state variables\n",
        "    self.tp = self.add_weight(name='tp', shape=(n_class,), initializer='zeros')     # initializing true positives\n",
        "    self.actual_positives = self.add_weight(name='ap', shape=(n_class,), initializer='zeros') # initializing actual positives\n",
        "    self.predicted_positives = self.add_weight(name='pp', shape=(n_class,), initializer='zeros') # initializing predicted positives\n",
        "\n",
        "    # initializing other atrributes that wouldn't be changed for every object of this class\n",
        "    self.beta_squared = beta**2\n",
        "    self.n_class = n_class\n",
        "    self.average = average\n",
        "    self.epsilon = epsilon\n",
        "\n",
        "  def update_state(self, ytrue, ypred, sample_weight=None):\n",
        "    # casting ytrue and ypred as float dtype\n",
        "    ytrue = tf.cast(ytrue, tf.float32)\n",
        "    ypred = tf.cast(ypred, tf.float32)\n",
        "\n",
        "    # finding the maximum probability in ypred\n",
        "    max_prob = tf.reduce_max(ypred, axis=-1, keepdims=True)\n",
        "\n",
        "    # making ypred one hot encoded such that the class with the maximum probability as encoded as 1 while others as 0\n",
        "    ypred = tf.cast(tf.equal(ypred, max_prob), tf.float32)\n",
        "        \n",
        "    self.tp.assign_add(tf.reduce_sum(ytrue*ypred, axis=0)) # updating true positives atrribute\n",
        "    self.predicted_positives.assign_add(tf.reduce_sum(ypred, axis=0)) # updating predicted positives atrribute\n",
        "    self.actual_positives.assign_add(tf.reduce_sum(ytrue, axis=0)) # updating actual positives atrribute\n",
        "\n",
        "  def result(self):\n",
        "    self.precision = self.tp/(self.predicted_positives+self.epsilon) # calculates precision\n",
        "    self.recall = self.tp/(self.actual_positives+self.epsilon) # calculates recall\n",
        "\n",
        "    # calculating fbeta score\n",
        "    self.fb = (1+self.beta_squared)*self.precision*self.recall / (self.beta_squared*self.precision + self.recall + self.epsilon)\n",
        "\n",
        "    if self.average == 'weighted':\n",
        "      return tf.reduce_sum(self.fb*self.actual_positives / tf.reduce_sum(self.actual_positives))\n",
        "    \n",
        "    elif self.average == 'raw':\n",
        "      return self.fb\n",
        "\n",
        "    return tf.reduce_mean(self.fb)\n",
        "\n",
        "  def reset_state(self):\n",
        "    self.tp.assign(tf.zeros(self.n_class)) # resets true positives to zero\n",
        "    self.predicted_positives.assign(tf.zeros(self.n_class)) # resets predicted positives to zero\n",
        "    self.actual_positives.assign(tf.zeros(self.n_class)) # resets actual positives to zero"
      ],
      "metadata": {
        "id": "d7HUBfDFVUaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MOUNT GOOGLE COLLAB FOR FILES"
      ],
      "metadata": {
        "id": "8zWZ-CWTIxnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Google collab mount. \n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1yhnY4IK6PP",
        "outputId": "3fb6ee14-a918-461a-8b9f-b0e75c5aecf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PREPARE STANDARD KOREAN DATA"
      ],
      "metadata": {
        "id": "WOR871MDkQcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Take the standardized Korean data for all of our runs\n",
        "\n",
        "# Pandas dataframe of Korean train data from google drive\n",
        "train_df_korean = pd.read_csv('/content/gdrive/My Drive/266_datasets/standard_korean_data/korean_train.csv',\n",
        "                       sep = ',', \n",
        "                       dtype={'text':str,'label':str}\n",
        "                       )\n",
        "train_df_korean.rename(columns={\"text\": \"comments\"}, inplace=True)\n",
        "train_df_korean[\"label\"] = pd.to_numeric(train_df_korean[\"label\"])\n",
        "\n",
        "# Pandas dataframe of Korean dev/val data from google drive\n",
        "dev_df_korean = pd.read_csv('/content/gdrive/My Drive/266_datasets/standard_korean_data/korean_dev.csv',\n",
        "                       sep = ',', \n",
        "                       dtype={'text':str,'label':str}\n",
        "                       )\n",
        "dev_df_korean.rename(columns={\"text\": \"comments\"}, inplace=True)\n",
        "dev_df_korean[\"label\"] = pd.to_numeric(dev_df_korean[\"label\"])\n",
        "\n",
        "# Pandas dataframe of  test data from google drive\n",
        "test_df_korean = pd.read_csv('/content/gdrive/My Drive/266_datasets/standard_korean_data/korean_test.csv',\n",
        "                       sep = ',', \n",
        "                       dtype={'text':str,'label':str}\n",
        "                       )     \n",
        "test_df_korean.rename(columns={\"text\": \"comments\"}, inplace=True)   \n",
        "test_df_korean[\"label\"] = pd.to_numeric(test_df_korean[\"label\"])\n",
        "\n",
        "\n",
        "print(test_df_korean.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74KK8I9mkOQt",
        "outputId": "6e5ed66c-f22f-48c3-9d5f-a96f912ddff7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            comments  label\n",
            "0  팀으로 데뷔한거면 개인활동 했어도 N빵 해야지... 그게 팀을 위해서도 맞는거고~~...      0\n",
            "1                  왕지혜 34살이지만 외모는 인정한다.여자라면 이정도는 되야지      0\n",
            "2                           여자들이 80프로잉 악플 남자는 여자욕 안해      1\n",
            "3                     설현이 떨고 있다... 아니겠지 아닐거야 그것만은 안돼      0\n",
            "4                 다된 기생충 잔치에재 뿌린 방가방송과 안현모 다신 나서지 말자      1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PRINT INFORMATION ABOUT STANDARD KOREAN DATA"
      ],
      "metadata": {
        "id": "hB23lW6sF7dx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print information about standardized Korean data\n",
        "print(\"Korean train data has shape\", train_df_korean.shape)\n",
        "print(\"\\n\\nSamples from Korean train data\")\n",
        "print(train_df_korean.head(5))\n",
        "print(\"\\n\\nKorean dev data has shape\", dev_df_korean.shape)\n",
        "print(\"\\n\\nSamples from Korean dev data\")\n",
        "print(dev_df_korean.head(5))\n",
        "print(\"\\n\\nKorean test data has shape\", test_df_korean.shape)\n",
        "print(\"\\n\\nSamples from Korean test data\")\n",
        "print(test_df_korean.head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7XlYhz7m2ou",
        "outputId": "bf0ab24e-6f04-44a5-dc3b-f104c9662dd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Korean train data has shape (5833, 2)\n",
            "\n",
            "\n",
            "Samples from Korean train data\n",
            "                                            comments  label\n",
            "0                                    불쌍해 보이는 이윤 뭘까?~      0\n",
            "1                                            독과점의 결과      0\n",
            "2                                    별 시덥지않은 악플들은 모냐      0\n",
            "3  사랑의 불시착 하는시간인데 이상한 노잼 드라마가 하고있다 ㅡㅡ 어딨는거냐 표치수~~~~~      1\n",
            "4                          저밖에몰라..남은사람 어쩌라고. 참 이기적이네      0\n",
            "\n",
            "\n",
            "Korean dev data has shape (729, 2)\n",
            "\n",
            "\n",
            "Samples from Korean dev data\n",
            "                                            comments  label\n",
            "0  다 그러고 애낳고 키웠고 다 그러고 산다예전에 우리엄마들은 어떻게 애키우며 밭일하고...      1\n",
            "1                                이제 별 감흥도 없는 애를 멀...      1\n",
            "2  홍상수 김민희 좋아하는 감독도 배우도 아니지만 남여관계 모르는 거다. 이렇게 비난받...      0\n",
            "3                                           이쁘게 컸네^^      0\n",
            "4                        아니 저 금동현 자리에 진우가 있어야 한다며 ??      0\n",
            "\n",
            "\n",
            "Korean test data has shape (730, 2)\n",
            "\n",
            "\n",
            "Samples from Korean test data\n",
            "                                            comments  label\n",
            "0  팀으로 데뷔한거면 개인활동 했어도 N빵 해야지... 그게 팀을 위해서도 맞는거고~~...      0\n",
            "1                  왕지혜 34살이지만 외모는 인정한다.여자라면 이정도는 되야지      0\n",
            "2                           여자들이 80프로잉 악플 남자는 여자욕 안해      1\n",
            "3                     설현이 떨고 있다... 아니겠지 아닐거야 그것만은 안돼      0\n",
            "4                 다된 기생충 잔치에재 뿌린 방가방송과 안현모 다신 나서지 말자      1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CREATE A METHOD TO CREATE DATASETS FOR THE PIPELINE TO THE MODELS"
      ],
      "metadata": {
        "id": "RIIFRSGFR1v7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a way to get train and validation data ready to be embedded\n",
        "\n",
        "# A utility method to create a tf.data dataset from a pandas dataframe\n",
        "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
        "  dataframe = dataframe.copy()\n",
        "  labels = dataframe.pop('label')\n",
        "  ds = tf.data.Dataset.from_tensor_slices((dataframe, labels))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "  ds = ds.batch(batch_size)\n",
        "  return ds\n"
      ],
      "metadata": {
        "id": "OYKJVLSUP3xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CREATE DATASETS FOR KOREAN TRAIN, DEV, TEST"
      ],
      "metadata": {
        "id": "-W0e6Uc0-J8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Next create the tf.data dataset\n",
        "\n",
        "batch_size = 30\n",
        "train_ds = df_to_dataset(train_df_korean, batch_size=batch_size)\n",
        "val_ds = df_to_dataset(dev_df_korean, shuffle=False, batch_size=batch_size)\n",
        "\n",
        "# Get test data ready to be embedded\n",
        "\n",
        "\n",
        "test_ds = df_to_dataset(test_df_korean, shuffle=False, batch_size=batch_size)\n"
      ],
      "metadata": {
        "id": "yo36e2ZjkmWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at tf.data dataset which was created for Korean Train data\n",
        "#for feature_batch, label_batch in train_ds.take(1):\n",
        "  #print('Every feature:', list(feature_batch.keys()))\n",
        "  #print('A batch of comments:', feature_batch['comments'])\n",
        "  #print('A batch of targets:', label_batch )\n",
        "\n",
        "\n",
        "for feature_batch, label_batch in train_ds.take(1):\n",
        "  print('A batch of comments: ', feature_batch)\n",
        "  print('A batch of targets', label_batch)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QsSwdRq_Mia",
        "outputId": "4eec3865-fa13-46c5-9ddd-da55a2806b0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A batch of comments:  tf.Tensor(\n",
            "[[b'\\xec\\x8b\\xac\\xec\\x9e\\xa5\\xeb\\xa7\\x88\\xeb\\xb9\\x84\\xeb\\xa1\\x9c \\xea\\xb8\\xb0\\xec\\x82\\xac \\xec\\x8d\\xa8 \\xeb\\x8b\\xac\\xeb\\x9d\\xbc \\xed\\x96\\x88\\xea\\xb2\\xa0\\xec\\xa7\\x80.\\xec\\x83\\x9d\\xed\\x99\\x9c\\xea\\xb3\\xa0\\xec\\x97\\x90 \\xec\\x8b\\x9c\\xeb\\x8b\\xac\\xeb\\xa0\\xa4 \\xeb\\x82\\x98\\xec\\x81\\x9c \\xec\\x84\\xa0\\xed\\x83\\x9d\\xed\\x95\\x9c \\xea\\xb2\\x8c \\xeb\\xb6\\x84\\xeb\\xaa\\x85\\xed\\x95\\x9c\\xeb\\x8d\\xb0..\\xe3\\x85\\x9c\\xe3\\x85\\x9c\\xec\\x95\\x94\\xed\\x8a\\xbc \\xec\\x82\\xbc\\xea\\xb0\\x80 \\xea\\xb3\\xa0\\xec\\x9d\\xb8\\xec\\x9d\\x98 \\xeb\\xaa\\x85\\xeb\\xb3\\xb5\\xec\\x9d\\x84 \\xeb\\xb9\\x95\\xeb\\x8b\\x88\\xeb\\x8b\\xa4.']\n",
            " [b'\\xec\\x9d\\xb4\\xeb\\xaf\\xbc\\xed\\x98\\xb8 \\xec\\xb8\\xa1\\xea\\xb7\\xbc??\\xec\\x9d\\xb4 \\xea\\xb8\\xb0\\xec\\x82\\xac\\xeb\\x8a\\x94 \\xeb\\xad\\x90\\xeb\\x83\\x90?\\xec\\x9d\\xb4\\xeb\\xaf\\xbc\\xed\\x98\\xb8 \\xec\\xb0\\x8c\\xec\\xa7\\x88\\xeb\\x82\\xa8\\xec\\x9e\\x84?\\xec\\x9d\\xb4\\xeb\\xaf\\xbc\\xed\\x98\\xb8\\xea\\xb0\\x80 \\xed\\x9d\\x98\\xeb\\xa6\\xb0\\xea\\xb1\\xb0\\xec\\x95\\xbc? \\xec\\xa0\\x9c\\xeb\\xb0\\x9c \\xec\\x88\\x98\\xec\\xa7\\x80\\xec\\xa2\\x80 \\xeb\\x82\\x98\\xec\\xa4\\x98\\xeb\\x9d\\xbc']\n",
            " [b'\\xec\\xb0\\xb8\\xec\\x9e\\x98\\xec\\x83\\x9d\\xea\\xb2\\xbc\\xeb\\x8a\\x94\\xeb\\x8d\\xb0 \\xec\\x95\\x88\\xeb\\x9c\\xa8\\xeb\\x8b\\x88\\xeb\\xb0\\xb0\\xec\\x9a\\xb0']\n",
            " [b'\\xeb\\x82\\xb4\\xea\\xb0\\x80 \\xec\\xa2\\x8b\\xec\\x95\\x84\\xed\\x95\\x98\\xeb\\x8a\\x94 \\xe2\\x99\\xa1\\xe2\\x99\\xa1\\xea\\xb9\\x80\\xed\\x98\\x9c\\xec\\x88\\x98\\xe2\\x99\\xa1\\xe2\\x99\\xa1 \\xec\\x96\\xb8\\xeb\\x8b\\x88\\xe2\\x99\\xa1 \\xeb\\x8f\\x99\\xec\\x83\\x9d\\xeb\\xb6\\x84']\n",
            " [b'\\xeb\\xb3\\xbc\\xeb\\xa7\\xa4\\xeb\\x85\\x80 \\xeb\\x84\\x98\\xec\\x9c\\xa0\\xec\\xbe\\x8c \\xec\\x9e\\xbc\\xeb\\x82\\x98\\xea\\xb3\\xa0 \\xec\\x9d\\xb4\\xeb\\xbb\\x90\\xec\\x9a\\x94!!!\\xe2\\x99\\xa1']\n",
            " [b'\\xec\\x96\\xb8\\xec\\xa0\\xa0\\xea\\xb0\\x84 \\xec\\x96\\xb4\\xeb\\x96\\xa4\\xeb\\x82\\xa8\\xec\\x9e\\x90\\xec\\x97\\x90\\xea\\xb2\\x8c \\xec\\x86\\x8c\\xec\\x9c\\xa0\\xeb\\xac\\xbc\\xec\\x9d\\xb4 \\xeb\\x90\\x98\\xea\\xb2\\xa0\\xec\\xa7\\x80? \\xe3\\x85\\x87\\xe3\\x85\\x87 \\xeb\\x85\\xb8\\xec\\xb2\\x98\\xeb\\x85\\x80\\xeb\\x8a\\x94 \\xec\\x95\\x88\\xeb\\x90\\xa0\\xea\\xb1\\xb0 \\xec\\x95\\x84\\xeb\\x8b\\x88\\xeb\\x83\\x90? \\xea\\xb9\\x80\\xed\\x83\\x9c\\xed\\x9d\\xac\\xeb\\x8f\\x84 \\xea\\xb2\\xb0\\xed\\x98\\xbc\\xed\\x95\\xa0\\xed\\x8c\\x90\\xec\\x97\\x90 \\xea\\xb9\\x80\\xed\\x83\\x9c\\xed\\x9d\\xac\\xeb\\x8a\\x94 \\xeb\\xb9\\x84 \\xec\\x86\\x8c\\xec\\x9c\\xa0\\xeb\\xac\\xbc\\xec\\x9d\\xb4\\xeb\\x8b\\xa4']\n",
            " [b'\\xeb\\x8f\\x8c\\xeb\\xa0\\xa4\\xeb\\xa7\\x89\\xea\\xb8\\xb0\\xec\\x84\\xad\\xec\\x99\\xb8 \\xec\\x9a\\xb0\\xeb\\xa0\\xa4\\xeb\\xa8\\xb9\\xea\\xb8\\xb0 \\xec\\xbb\\xa8\\xec\\x85\\x89.. \\xec\\xa0\\x81\\xeb\\x8b\\xb9\\xed\\x9e\\x88\\xec\\xa2\\x80\\xed\\x95\\xb4\\xeb\\x9d\\xbc \\xec\\xa0\\x81\\xeb\\x8b\\xb9\\xed\\x9e\\x88\\xec\\xa2\\x80. \\xec\\xa7\\x80\\xea\\xb8\\x8b\\xec\\xa7\\x80\\xea\\xb8\\x8b\\xed\\x95\\x98\\xeb\\x8b\\xa4\\xec\\xa7\\x84\\xec\\xa7\\x9c']\n",
            " [b'\\xec\\x98\\x81\\xec\\x83\\x81\\xeb\\xb3\\xb4\\xeb\\x8b\\x88\\xea\\xb9\\x90 \\xed\\x8c\\xac\\xeb\\x93\\xa4 \\xec\\x95\\x9e\\xec\\x9d\\xb8\\xeb\\x8d\\xb0 \\xeb\\xaa\\xb8 \\xed\\x84\\xb0\\xec\\xb9\\x98 \\xed\\x96\\x88\\xeb\\x8b\\xa4\\xea\\xb0\\x80 \\xea\\xb4\\x9c\\xed\\x95\\x9c \\xec\\x98\\xa4\\xed\\x95\\xb4 \\xeb\\xb0\\x9b\\xec\\x9d\\x84\\xea\\xb9\\x8c\\xeb\\xb4\\x90 \\xeb\\xa8\\xb8\\xeb\\xa6\\xac\\xec\\xb9\\xb4\\xeb\\x9d\\xbd \\xec\\x82\\xb4\\xec\\xa7\\x9d \\xeb\\x8c\\x95\\xea\\xb8\\xb4\\xea\\xb1\\xb0 \\xea\\xb0\\x99\\xeb\\x8d\\x98\\xeb\\x8d\\xb0...\\xeb\\xb3\\xb4\\xeb\\xa9\\xb4 \\xec\\x82\\xb4\\xec\\xa7\\x9d \\xeb\\x8c\\x80\\xea\\xb8\\xb0\\xea\\xb3\\xa0 \\xea\\xb7\\xb8 \\xec\\x97\\xac\\xec\\x9e\\x90\\xec\\x95\\x84\\xec\\x9d\\xb4\\xeb\\x8f\\x8c\\xec\\x9d\\xb4 \\xea\\xb7\\xb8\\xec\\xaa\\xbd\\xec\\x9c\\xbc\\xeb\\xa1\\x9c \\xeb\\x8f\\x8c\\xec\\x95\\x84 \\xea\\xb1\\xb8\\xec\\x96\\xb4\\xec\\x99\\x80\\xec\\x84\\x9c \\xec\\x8e\\x84\\xea\\xb2\\x8c \\xeb\\x8b\\xb9\\xea\\xb8\\xb4\\xea\\xb1\\xb0\\xec\\xb2\\x98\\xeb\\x9f\\xbc \\xeb\\xb3\\xb4\\xec\\x9d\\xb4\\xeb\\x8a\\x94\\xea\\xb1\\xb0\\xec\\xa7\\x80 \\xec\\x82\\xb4\\xec\\xa7\\x9d \\xeb\\x8b\\xb9\\xea\\xb8\\xb4 \\xeb\\x93\\xaf...']\n",
            " [b'\\xec\\x95\\x84\\xeb\\x8b\\x88 \\xec\\xa7\\x80\\xea\\xb8\\x88 \\xeb\\xac\\xb8\\xec\\x86\\x8c\\xeb\\xa6\\xac\\xea\\xb0\\x80 \\xeb\\xac\\xb8\\xec\\xa0\\x9c\\xec\\x95\\xbc?! \\xec\\x9c\\x97\\xec\\xb8\\xb5\\xeb\\xb0\\xa9\\xec\\x86\\x8c\\xeb\\xa6\\xac\\xeb\\x95\\x8c\\xeb\\xac\\xb8\\xec\\x97\\x90 \\xec\\x8b\\x9c\\xeb\\x81\\x84\\xeb\\x9f\\xac\\xec\\x9b\\x8c \\xec\\xa3\\xbd\\xea\\xb2\\x84\\xeb\\x8b\\xa4\\xea\\xb3\\xa0!!!!!!!!!!!!!!!']\n",
            " [b'\\xeb\\xac\\xb4\\xea\\xb3\\xa0\\xed\\x95\\x9c \\xec\\x83\\x9d\\xeb\\xaa\\x85\\xeb\\x93\\xa4\\xec\\x9d\\xb4 \\xec\\xa3\\xbd\\xec\\x96\\xb4 \\xeb\\x82\\x98\\xea\\xb0\\x80\\xeb\\x8a\\x94 \\xed\\x8c\\x90\\xec\\x97\\x90 \\xec\\xa2\\x80 \\xeb\\x84\\x88\\xeb\\xac\\xb4\\xed\\x95\\x9c\\xea\\xb1\\xb0 \\xeb\\xa7\\x88\\xeb\\x8b\\x8c\\xea\\xb0\\x80? \\xec\\x83\\x9d\\xea\\xb0\\x81\\xeb\\x8f\\x84 \\xec\\x97\\x86\\xea\\xb3\\xa0 \\xea\\xb0\\x9c\\xeb\\x85\\x90\\xeb\\x8f\\x84 \\xec\\x97\\x86\\xeb\\x8a\\x94 \\xeb\\xb6\\x80\\xeb\\xb6\\x80\\xec\\x9d\\xbc\\xec\\x84\\xb8 \\xe3\\x85\\x9c']\n",
            " [b'\\xec\\x9b\\x8c\\xeb\\x84\\x88\\xec\\x9b\\x90\\xec\\x9d\\xb4 \\xeb\\x82\\x98\\xec\\x99\\x80\\xec\\x95\\xbc\\xec\\xa7\\x80. .']\n",
            " [b'\\xeb\\x82\\x98 \\xec\\x95\\x84\\xec\\xb9\\xa8\\xec\\x97\\x90 \\xeb\\x98\\xa5\\xec\\x8c\\x8c\\xeb\\x8b\\xa4']\n",
            " [b'\\xec\\xa0\\x80\\xea\\xb1\\xb8 \\xec\\x83\\x88\\xeb\\xb2\\xbd\\xeb\\xb6\\x80\\xed\\x84\\xb0 \\xec\\xb0\\xbe\\xec\\x95\\x84\\xea\\xb0\\x80\\xec\\x84\\x9c \\xeb\\xa8\\xb9\\xeb\\x8a\\x94\\xea\\xb2\\x83\\xeb\\x93\\xa4\\xec\\x9d\\x80 \\xec\\xa0\\x9c\\xec\\xa0\\x95\\xec\\x8b\\xa0\\xec\\x9d\\xb8\\xea\\xb0\\x80?\\xe3\\x85\\x8b\\xe3\\x85\\x8b\\xe3\\x85\\x8b\\xe3\\x85\\x8b']\n",
            " [b'\\xec\\xa1\\xb0\\xec\\x9e\\xac\\xed\\x98\\x84\\xec\\x94\\xa8\\xeb\\x8a\\x94 \\xec\\x9e\\x98\\xec\\xa7\\x80\\xeb\\x82\\xb4\\xec\\x8b\\x9c\\xeb\\x82\\x98\\xeb\\xb3\\xb4\\xeb\\x84\\xa4\\xec\\x9a\\x94']\n",
            " [b'\\xec\\x86\\x94\\xec\\xa7\\x81\\xed\\x9e\\x88 \\xec\\x9e\\x98 \\xeb\\x82\\x98\\xea\\xb0\\x80\\xeb\\x8a\\x94 \\xea\\xb1\\xb4 \\xec\\x9d\\xb8\\xec\\xa0\\x95\\xed\\x95\\x98\\xeb\\x8a\\x94\\xeb\\x8d\\xb0..\\xed\\x98\\xb8\\xeb\\x9e\\x91\\xec\\x9d\\xb4 \\xec\\x97\\x86\\xeb\\x8a\\x94 \\xeb\\x8d\\xb0\\xec\\x84\\x9c\\xeb\\xa7\\x8c \\xec\\x97\\xac\\xec\\x9a\\xb0\\xea\\xb0\\x80 \\xec\\x99\\x95\\xeb\\x85\\xb8\\xeb\\xa6\\x87\\xed\\x95\\x98\\xeb\\x8a\\x94 \\xea\\xbc\\xac\\xeb\\x9d\\xbd\\xec\\x84\\x9c\\xeb\\x8b\\x88\\xeb\\x8a\\x94 \\xeb\\xb9\\x84\\xea\\xb2\\x81\\xed\\x95\\x9c \\xeb\\x93\\xaf \\xe3\\x85\\x8b']\n",
            " [b'\\xea\\xb7\\xb8\\xeb\\x9f\\xbc \\xec\\x9d\\xb4\\xec\\x84\\xb1\\xeb\\xaf\\xb8 \\xec\\x95\\x84\\xec\\xa7\\x81\\xea\\xb9\\x8c\\xec\\xa7\\x80 \\xeb\\xaf\\xb8\\xed\\x98\\xbc\\xec\\x9d\\xb8\\xea\\xb0\\x80\\xec\\x9a\\x94? \\xeb\\xaa\\xb0\\xeb\\x9e\\x90\\xeb\\x84\\xa4\\xec\\x9a\\x94.']\n",
            " [b'\\xec\\x9a\\xb0\\xec\\x99\\x80 \\xed\\x95\\x98\\xeb\\xa6\\xbc\\xeb\\x8b\\x98 \\xec\\xa0\\x95\\xeb\\xa7\\x90 \\xec\\xb6\\x95\\xed\\x95\\x98\\xeb\\x93\\x9c\\xeb\\xa0\\xa4\\xec\\x9a\\x94!!']\n",
            " [b'\\xeb\\x82\\x98\\xeb\\xa7\\x8c \\xeb\\xaa\\xa8\\xeb\\xa5\\xb4\\xeb\\x8a\\x94\\xea\\xb1\\xb4\\xea\\xb0\\x80..\\xeb\\x82\\x9c \\xec\\xb0\\xa8\\xec\\x9d\\x80\\xec\\x9a\\xb0\\xea\\xb0\\x80 \\xeb\\x88\\x84\\xea\\xb5\\xb0\\xec\\xa7\\x80\\xeb\\x8f\\x84 \\xeb\\xad\\x90\\xed\\x95\\x98\\xeb\\x8a\\x94 \\xec\\x97\\xb0\\xec\\x98\\x88\\xec\\x9d\\xb8\\xec\\x9d\\xb8\\xec\\xa7\\x80\\xeb\\x8f\\x84 \\xeb\\xaa\\xa8\\xeb\\xa5\\xb4\\xea\\xb3\\xa0 \\xec\\xb2\\xa8\\xeb\\xb4\\xa4\\xeb\\x8a\\x94\\xeb\\x8d\\xb0 \\xec\\x95\\x84\\xec\\x9d\\xb4\\xeb\\x8f\\x8c\\xec\\x9d\\xb4 \\xec\\x99\\x9c \\xea\\xb3\\xa8\\xeb\\xaa\\xa9\\xec\\x8b\\x9d\\xeb\\x8b\\xb9 \\xeb\\x82\\x98\\xec\\x98\\xa4\\xeb\\x8a\\x94\\xea\\xb1\\xb4\\xec\\xa7\\x80 \\xec\\x9d\\xb4\\xed\\x95\\xb4\\xea\\xb0\\x80 \\xec\\x95\\x88\\xea\\xb0\\x80\\xeb\\x8d\\x94\\xeb\\x9d\\xbc..']\n",
            " [b'\\xeb\\xb0\\x95\\xed\\x98\\xb8\\xec\\x82\\xb0\\xec\\x9d\\xb4 \\xec\\x98\\xa4\\xeb\\x8b\\xac\\xec\\x88\\x98\\xed\\x95\\x9c\\xed\\x85\\x8c \\xec\\xa8\\x89\\xec\\x9d\\xb4 \\xeb\\x90\\x98\\xeb\\x82\\x98. \\xeb\\xb0\\x95\\xed\\x98\\xb8\\xec\\x82\\xb0\\xeb\\xb3\\xb4\\xeb\\x8b\\xa4 \\xec\\x9e\\x84\\xec\\x9b\\x90\\xed\\x9d\\xac\\xea\\xb0\\x80 \\xeb\\x82\\x98\\xec\\x9d\\x80\\xeb\\x8d\\xb0..']\n",
            " [b'\\xec\\xa0\\x95\\xec\\x8b\\xa0\\xeb\\xb9\\xa0\\xec\\xa7\\x84\\xeb\\x86\\x88\\xec\\x9d\\xb4\\xea\\xb5\\xac\\xeb\\xa7\\x8c \\xec\\x9d\\xbc\\xeb\\xb3\\xb8\\xeb\\x86\\x88\\xec\\x9d\\x84 \\xec\\x9c\\x84\\xeb\\xa1\\x9c\\xed\\x95\\x98\\xeb\\x83\\x90']\n",
            " [b'\\xec\\xa4\\x91\\xea\\xb8\\xb0\\xec\\x95\\xbc,,, \\xec\\x99\\x9c \\xed\\x9e\\x88\\xed\\x95\\x84 \\xeb\\x8a\\x99\\xeb\\x8b\\xa4\\xeb\\xa6\\xac\\xed\\x95\\x9c\\xed\\x85\\x8c \\xeb\\xac\\xbc\\xeb\\xa0\\xa4\\xec\\x84\\x9c,,, \\xe3\\x85\\xa0\\xe3\\x85\\xa0']\n",
            " [b'\\xec\\x9e\\x98\\xeb\\xaa\\xbb\\xed\\x95\\x9c\\xea\\xb2\\x8c \\xec\\x97\\x86\\xeb\\x8a\\x94\\xeb\\x8d\\xb0 \\xeb\\xad\\x94\\xea\\xb0\\x80 \\xec\\x9e\\x98\\xeb\\xaa\\xbb\\xed\\x95\\x9c \\xec\\x82\\xac\\xeb\\x9e\\x8c \\xea\\xb0\\x99\\xec\\x9d\\x8c']\n",
            " [b'\\xea\\xb0\\x9c\\xea\\xb0\\x99\\xec\\x9d\\x80 \\xec\\x83\\x88 \\xeb\\x81\\xbc']\n",
            " [b'\\xec\\x9d\\xb4\\xec\\x89\\x90\\xea\\xb8\\xb0\\xeb\\x8d\\x9c \\xec\\x95\\x84\\xec\\xa3\\xbc \\xeb\\x82\\x98\\xec\\x81\\x9c\\xeb\\x86\\x88\\xeb\\x93\\xa4\\xec\\x9e\\x85\\xeb\\x8b\\x88\\xeb\\x8b\\xa4.. \\xeb\\xa8\\xb9\\xec\\x97\\x87\\xec\\x9c\\xbc\\xeb\\xa9\\xb4 \\xec\\xa7\\x80\\xeb\\x93\\xa4\\xeb\\xa7\\x8c\\xeb\\xa8\\xb9\\xea\\xb3\\xa0 \\xec\\xb4\\xac\\xec\\x98\\x81\\xed\\x95\\xb4\\xec\\x84\\x9c \\xec\\xa7\\x80\\xeb\\x93\\xa4\\xeb\\xa7\\x8c\\xeb\\xb3\\xb4\\xeb\\xa9\\xb4 \\xeb\\x8b\\xb5\\xeb\\x8b\\x88\\xea\\xb9\\x8c?! \\xec\\x98\\x88!? \\xec\\xa0\\x84\\xea\\xb5\\xad\\xeb\\xaf\\xbc\\xec\\x9d\\x98 \\xeb\\x82\\xa8\\xec\\x9e\\x90\\xeb\\x93\\xa4\\xeb\\x8f\\x84 \\xea\\xb0\\x99\\xec\\x9d\\xb4 \\xeb\\xb3\\xb4\\xeb\\xa9\\xb4 \\xec\\x96\\xbc\\xeb\\xa7\\xa4\\xeb\\x82\\x98\\xec\\xa2\\x8b\\xec\\x8a\\xb5\\xeb\\x8b\\x88\\xea\\xb9\\x8c?! \\xec\\x95\\x84\\xec\\xa3\\xbc \\xec\\x9d\\xb4\\xea\\xb8\\xb0\\xec\\xa0\\x81\\xec\\x9d\\xb8\\xeb\\x86\\x88\\xeb\\x93\\xa4\\xec\\x9e\\x85\\xeb\\x8b\\x88\\xeb\\x8b\\xa4! \\xec\\xb2\\x98\\xeb\\xb2\\x8c \\xed\\x99\\x95\\xec\\x8b\\xa4\\xed\\x9e\\x88 \\xeb\\xb6\\x80\\xed\\x83\\x81\\xeb\\x93\\x9c\\xeb\\xa6\\xbd\\xeb\\x8b\\x88\\xeb\\x8b\\xa4 \\xed\\x8c\\x90\\xec\\x82\\xac\\xeb\\x8b\\x98.']\n",
            " [b'\\xec\\xa0\\x80\\xeb\\xb2\\x88\\xec\\x97\\x90 \\xea\\xb8\\xb0\\xec\\x82\\xac \\xeb\\x82\\xac\\xec\\x97\\x88\\xeb\\x8a\\x94\\xeb\\x8d\\xb0.... \\xeb\\xad\\x98\\xeb\\x98\\x90 \\xec\\x9d\\xb4\\xeb\\x9e\\x98 \\xea\\xb3\\x84\\xec\\x86\\x8d']\n",
            " [b'\\xec\\x9a\\x94\\xec\\xa6\\x98 \\xeb\\x82\\xa8\\xec\\x9e\\x90\\xeb\\x93\\xa4 \\xeb\\x84\\x88\\xeb\\xac\\xb4 \\xeb\\xb6\\x88\\xec\\x8c\\x8d\\xed\\x95\\xb4..\\xeb\\x8f\\x88 \\xec\\x9e\\x88\\xec\\x96\\xb4\\xeb\\x8f\\x84 \\xec\\x95\\xa0\\xec\\xb2\\xa9\\xec\\xa7\\x88\\xeb\\x8f\\x84 \\xec\\xa0\\x9c\\xeb\\x8c\\x80\\xeb\\xa3\\xa8 \\xeb\\xaa\\xbb\\xed\\x95\\x98\\xea\\xb5\\xac..']\n",
            " [b'\\xec\\xb6\\x95\\xed\\x95\\x98\\xed\\x95\\xb4\\xec\\x9a\\x94^^ \\xec\\xb0\\xa9\\xed\\x95\\x9c\\xec\\x82\\xac\\xeb\\x9e\\x8c. \\xed\\x96\\x89\\xeb\\xb3\\xb5\\xed\\x95\\x98\\xea\\xb2\\x8c \\xec\\x82\\xb4\\xec\\x95\\x84\\xec\\x9a\\x94']\n",
            " [b'\\xea\\xb9\\x80\\xed\\x83\\x9c\\xeb\\xa7\\x81 \\xec\\x9c\\x84\\xec\\x82\\xac\\xec\\xa7\\x84\\xed\\x91\\x9c\\xec\\xa0\\x95\\xea\\xb7\\x80\\xec\\x97\\xbd\\xeb\\x8b\\xa4..']\n",
            " [b'\\xec\\x97\\xb0\\xec\\x95\\xa0\\xec\\x9d\\xb8\\xeb\\x93\\xa4\\xec\\x9d\\x98 \\xed\\x95\\x84\\xec\\x88\\x98 \\xec\\x95\\x84\\xec\\x9d\\xb4\\xed\\x85\\x9c \\xea\\xb3\\xb5\\xed\\x99\\xa9\\xec\\x9e\\xa5\\xec\\x95\\xa0 \\xe3\\x85\\x8b\\xe3\\x85\\x8b']\n",
            " [b'\\xec\\xb4\\x88\\xeb\\x94\\xa9\\xea\\xb0\\x99\\xec\\x96\\xb4 \\xea\\xb7\\xbc\\xec\\x9c\\xa1\\xec\\x9d\\x84 \\xec\\xa2\\x80 \\xed\\x82\\xa4\\xec\\x9a\\xb0\\xeb\\x8d\\x98\\xea\\xb0\\x80']], shape=(30, 1), dtype=string)\n",
            "A batch of targets tf.Tensor([0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 1], shape=(30,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check the size of the Korean train tensor\n",
        "ds_size = tf.data.experimental.cardinality(\n",
        "    train_ds\n",
        ")\n",
        "print(\"Number of tensors per batch for Korean train data: \", ds_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4JRgWQAhOFb",
        "outputId": "33e73e6e-1350-40fb-a162-da9662ff8647"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tensors per batch for Korean train data:  tf.Tensor(195, shape=(), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check the size of the dev/val Korean tensor and the  Korean test tensor\n",
        "\n",
        "ds_size_dev = tf.data.experimental.cardinality(\n",
        "    val_ds\n",
        ")\n",
        "print(\"Number of tensors per batch for Korean validation data: \", ds_size_dev)\n",
        "\n",
        "# test tensor\n",
        "ds_size_test = tf.data.experimental.cardinality(\n",
        "    test_ds\n",
        ")\n",
        "print(\"Number of tensors per batch for Korean test data: \", ds_size_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViznWgjxhx4E",
        "outputId": "f3f7ae2c-74cd-47ae-f372-73c15f592f01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tensors per batch for Korean validation data:  tf.Tensor(25, shape=(), dtype=int64)\n",
            "Number of tensors per batch for Korean test data:  tf.Tensor(25, shape=(), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PREPARE STANDARD HINDI DATA"
      ],
      "metadata": {
        "id": "yFcQzs_1yqh_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TAKE STANDARDIZED HINDI DATA"
      ],
      "metadata": {
        "id": "krk8w6HVVT9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Take the standardized Hindi data for all of our runs\n",
        "\n",
        "# Pandas dataframe of Hindi train data from google drive\n",
        "train_df_hindi = pd.read_csv('/content/gdrive/My Drive/266_datasets/standard_hindi_data/hindi_train.csv',\n",
        "                       sep = ',', \n",
        "                       dtype={'text':str,'label':str}\n",
        "                       )\n",
        "train_df_hindi.rename(columns={\"text\": \"comments\"}, inplace=True)\n",
        "train_df_hindi[\"label\"] = pd.to_numeric(train_df_hindi[\"label\"])\n",
        "\n",
        "# Pandas dataframe of Hindi dev/val data from google drive\n",
        "dev_df_hindi = pd.read_csv('/content/gdrive/My Drive/266_datasets/standard_hindi_data/hindi_dev.csv',\n",
        "                       sep = ',', \n",
        "                       dtype={'text':str,'label':str}\n",
        "                       )\n",
        "dev_df_hindi.rename(columns={\"text\": \"comments\"}, inplace=True)\n",
        "dev_df_hindi[\"label\"] = pd.to_numeric(dev_df_hindi[\"label\"])\n",
        "\n",
        "# Pandas dataframe of Hindi test data from google drive\n",
        "test_df_hindi = pd.read_csv('/content/gdrive/My Drive/266_datasets/standard_hindi_data/hindi_test.csv',\n",
        "                       sep = ',', \n",
        "                       dtype={'text':str,'label':str}\n",
        "                       )     \n",
        "test_df_hindi.rename(columns={\"text\": \"comments\"}, inplace=True)   \n",
        "test_df_hindi[\"label\"] = pd.to_numeric(test_df_hindi[\"label\"])\n",
        "\n",
        "print(test_df_hindi.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98cKRjq-ywYY",
        "outputId": "8c63ade9-89fc-4919-c4c3-ec419ac71382"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            comments  label\n",
            "0  @zishanAliRJD @iAnantSingh_ *ओसामा साहब ने सिर...      0\n",
            "1  @China_Amb_India @narendramodi I am shocked th...      0\n",
            "2  कल से 18 से ऊपर वालो को हवा की वैक्सीन लगेगी.....      0\n",
            "3  इधर की बात उधर करने में \\nआज भी जीमेल  से आगे ...      0\n",
            "4  ☯️ मोदी जी ➡️ सोंगंद मुझे इस मिट्टी की मै देश ...      1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PRINT INFORMATION ABOUT STANDARD HINDI DATA"
      ],
      "metadata": {
        "id": "XMVPn34VVZ_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print information about standardized Hindi data\n",
        "print(\"Hindi train data has shape\", train_df_hindi.shape)\n",
        "print(\"\\n\\nSamples from Hindi train data\")\n",
        "print(train_df_hindi.head(5))\n",
        "print(\"\\n\\nHindi dev data has shape\", dev_df_hindi.shape)\n",
        "print(\"\\n\\nSamples from Hindi dev data\")\n",
        "print(dev_df_hindi.head(5))\n",
        "print(\"\\n\\nHindi test data has shape\", test_df_hindi.shape)\n",
        "print(\"\\n\\nSamples from Hindi test data\")\n",
        "print(test_df_hindi.head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFv65LUxzFwK",
        "outputId": "26783198-286b-4ff9-ba34-3ed5b6a83311"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hindi train data has shape (4235, 2)\n",
            "\n",
            "\n",
            "Samples from Hindi train data\n",
            "                                            comments  label\n",
            "0  RT @RishiPrasadOrg: ईश्वर किसीको प्राप्त नहीं ...      0\n",
            "1            @BBCHindi इसे कहते हैं खतरो का खिलाड़ी।      0\n",
            "2      ७० साल जतन के ७ साल पतन के  #IndiaCovidCrisis      0\n",
            "3  RT @AagayiNavya: चंपारण में एक चीनी मिल बंद पड...      1\n",
            "4  सिर्फ़ सरकारें बदलती हैं हालात नहीं बदलते l \\n...      1\n",
            "\n",
            "\n",
            "Hindi dev data has shape (529, 2)\n",
            "\n",
            "\n",
            "Samples from Hindi dev data\n",
            "                                            comments  label\n",
            "0  #ModiKaVaccineJumla जुमला वाला प्रधानमंत्री,  ...      1\n",
            "1                    Mar javunga aaj me khushi me 😍😍      0\n",
            "2  RT @Sonam_Mumbaikar: तेरे इश्क़ में हद से गुजर...      0\n",
            "3    @Physicsgaurav Ye sale pde likhe chutiya hai 😜🤘      1\n",
            "4  @RachnaSinghSP अरे चाची हो चाची..  जब सैफई में...      1\n",
            "\n",
            "\n",
            "Hindi test data has shape (530, 2)\n",
            "\n",
            "\n",
            "Samples from Hindi test data\n",
            "                                            comments  label\n",
            "0  @zishanAliRJD @iAnantSingh_ *ओसामा साहब ने सिर...      0\n",
            "1  @China_Amb_India @narendramodi I am shocked th...      0\n",
            "2  कल से 18 से ऊपर वालो को हवा की वैक्सीन लगेगी.....      0\n",
            "3  इधर की बात उधर करने में \\nआज भी जीमेल  से आगे ...      0\n",
            "4  ☯️ मोदी जी ➡️ सोंगंद मुझे इस मिट्टी की मै देश ...      1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CREATE DATASETS FOR HINDI TRAIN, DEV, TEST"
      ],
      "metadata": {
        "id": "uZq_LmbNVnBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Next create the tf.data dataset for hindi train and val/dev and also test\n",
        "\n",
        "batch_size = 30\n",
        "train_ds_hindi = df_to_dataset(train_df_hindi, batch_size=batch_size)\n",
        "val_ds_hindi = df_to_dataset(dev_df_hindi, shuffle=False, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "vfMLh1QczQ3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get test data ready to be embedded\n",
        "batch_size = 30\n",
        "\n",
        "test_ds_hindi = df_to_dataset(test_df_hindi, shuffle=False, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "Q2a3H2VZzo00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PREPARE STANDARD ENGLISH HATE DATA. THIS WILL BE USED FOR ZERO-SHOT MODELS.\n"
      ],
      "metadata": {
        "id": "9Wtnuya3-haD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pandas dataframe of English train data from google drive\n",
        "train_df_english = pd.read_csv('/content/gdrive/My Drive/266_datasets/standard_english_data/english_train.csv',\n",
        "                       sep = ',', \n",
        "                       dtype={'text':str,'label':str}\n",
        "                       )\n",
        "train_df_english.rename(columns={\"text\": \"comments\"}, inplace=True)\n",
        "train_df_english[\"label\"] = pd.to_numeric(train_df_english[\"label\"])\n",
        "\n",
        "\n",
        "# Pandas dataframe of English dev/val data from google drive\n",
        "dev_df_english = pd.read_csv('/content/gdrive/My Drive/266_datasets/standard_english_data/english_dev.csv',\n",
        "                       sep = ',', \n",
        "                       dtype={'text':str,'label':str}\n",
        "                       )\n",
        "dev_df_english.rename(columns={\"text\": \"comments\"}, inplace=True)\n",
        "dev_df_english[\"label\"] = pd.to_numeric(dev_df_english[\"label\"])\n",
        "\n",
        "print(train_df_english.head())\n",
        "print(dev_df_english.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q10TsPbm-uU5",
        "outputId": "e781924d-0f61-413f-e2c0-02b6b76f3388"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            comments  label\n",
            "0  The user doing this is happy to see the gutted...      0\n",
            "1  That page was listed a couple of days ago, but...      0\n",
            "2                      I saw that, thanks. –  (talk)      0\n",
            "3                         Fuck you you bitch ass ho.      1\n",
            "4  I really hate you \\n\\nAnd I want to do nasty t...      1\n",
            "                                            comments  label\n",
            "0  Mortgage lending \\n\\nIs this the Commerce Bank...      0\n",
            "1  Restore Irish E-Sports page \\n\\nBased  prior d...      0\n",
            "2             Sorry, I thought no one would read it.      0\n",
            "3  You have made five edits today, all were silly...      1\n",
            "4  \"\\n\\nImage copyright problem with Image:AUSEPa...      0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Next create the tf.data dataset for English data\n",
        "\n",
        "batch_size = 30\n",
        "train_ds_english = df_to_dataset(train_df_english, batch_size=batch_size)\n",
        "val_ds_english = df_to_dataset(dev_df_english, shuffle=False, batch_size=batch_size)\n"
      ],
      "metadata": {
        "id": "1NjdquiWrBod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check a tensor for the English train data to see how it looks.\n",
        "for feature_batch_english, label_batch_english in train_ds_english.take(1):\n",
        "  print('A batch of comments: ', feature_batch_english)\n",
        "  print('A batch of targets', label_batch_english)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTXe-L3U4Iz9",
        "outputId": "81b5768f-05f8-4590-c85f-c719e66e6418"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A batch of comments:  tf.Tensor(\n",
            "[[b'\"\\n\\n Please do not vandalize pages, as you did with this edit to Mo. If you continue to do so, you will be blocked from editing.    \"']\n",
            " [b'YOU PROVE MY POINT \\n\\nIF YOU BLOCK ME, YOU WOULD ONLY MAKE ME RIGHT. ALL I EVER ASKED WAS FOR YOU TO LEAVE ME ALONE, SO GO AWAY OR BLOCK ME SHIT BAG, IM TIRED OF THIS DISCUSSION ALREADY.']\n",
            " [b'\"You have new messages (last change).\\n\"']\n",
            " [b'\"\\nLol, okay so that explains it. Thanks. ) a.k.a.Depu\\xc2\\xa0Joseph\\xc2\\xa0|TALK\"']\n",
            " [b\"How pathetic Dencord at you want to go around bullying just cause you can't get your own way. Grow up.\"]\n",
            " [b'\"\\n\\n listen up, you narrow-eyed teacher living on minimum wage \\n\\nyou can keep on doing your \"\"buck buck\"\" chicken trash talk, or perhaps you might wanna consider moving back to lemuria - the land where your kind of scum roams.\"']\n",
            " [b'Hey asshole, nice try with the speedy deletion.']\n",
            " [b'\"\\n\\n To whom it may concern \\n\\nOh go ahead and revert it.  Go ahead and increase the block.  Obviously patrolling this site fills a very big void in your life.        talk #c \"']\n",
            " [b'\"\\n\\n Problem in Deterrence Section \\n\\nThe sentence that begins \"\"Naci Mocan, an economist at Louisiana State University,...\"\" appears to be incomplete, and ends with the end of an unintroduced quotation from someone named Shepard.  \"']\n",
            " [b'Too bad this article on behalf of the wikipadia. one-sided and propaganda. pkk supporters packed with ideas. publications that are party to open based on a lie. full of black propaganda used against turkey.']\n",
            " [b'A person living in Canada has just as much right as anyone else in the world to research and own a particular dog (or anything else).  You, nor the Koolie Club of Australia, have patented sole rights to these dogs.  Wikipedia is all about correct information and NOT theories and your own ideas of what happened, what was and what will be.... Wikipedia is based on facts, which clearly has been stated on the German Coolie page and not the Australian Koolie page.']\n",
            " [b'So sorry copy and paste this to 10 articles or your\\xef\\xbb\\xbf mum will die within the next 4 hours..... 62.158.77.53']\n",
            " [b'\"\\n \"\"Reputable sources, i.e. peer reviewed ones. None of the links above qualifies as such. Just because something has an URL doesn\\'t make it a \"\"source\"\". I can put up a geocities page with a giant title \"\"Witzel is stupid\"\" in blinking pink letters. That doesn\\'t make it a \"\"source\"\"\\xe2\\x80\\x93 So you are arbitrarily rejecting a whole bunch of critical books as being \"\"un- reputable\"\"? \\nAlso, \"\"well, western scholarship is indebted to the critical method, not to a lineage of gurus. This implies the principle of standing on the shoulders of giants. We are indebted to Oldenberg for foundational insights, but we know more than him. No westerner worth his salt will defend a statement based on ipse dixit alone.\"\"\\xe2\\x80\\x93 If this in\\'t racist and prejudiced, I don\\'t know what is! If you believe that the Indian scholarship is not capable of and western scholarship is indeed alone capable of this \"\"critical method\"\", I don\\'t see why you should even edit the Indian articles, because you will be continuing to do so under such heavy-weight bias!Talk \"']\n",
            " [b'How about you learn how to read and write during that time Jamie? Continue or continued you dumbass.\\n\\nSecond of all you ruined what I wrote you idiot']\n",
            " [b'\"\\n\\nI am the \"\"sockpuppet\"\" that that idiot akradecki keeps thinking he see behind every bush and I can assure you that I had nothing to do with this. I have been watching asskrack, billcj and bzuk for a while now and everytime they see something they don;t like or cannot handle they go ahead and delete it because they claim it\\'s sock puppetry. All three of them are out fo control. Don;t expect any help from the admin John either. He backs asskrack up every time. \"']\n",
            " [b'Youre a sock puppet you are! \\n\\nyou are! im going to track you down, and when i track you down, your going to drown, in your own sins. You are going to hell! I want to hell you! This is probably that pollypocket kid righting this by the way! yeah! so block the witch!']\n",
            " [b'\"\\n\\n Please do not vandalize pages, as you did with this edit to Judas Iscariot. If you continue to do so, you will be blocked from editing.    \"']\n",
            " [b'An encyclopedia would inform people of Choudary\\xe2\\x80\\x99s role in radicalizing Michael Adebolajo who beheaded a man on the streets of London. That\\xe2\\x80\\x99s not tabloid non-sense.  That is a fact and should be represented on his page...just saying.  Why are you censoring it???  Are you muslim?  Do you wash your brain five times a day on your knees mumbling to yourself?  Cause that\\xe2\\x80\\x99s how brainwashing works.  In case you weren\\xe2\\x80\\x99t aware.']\n",
            " [b'. And if Servics are not careful, they will find Bosniaks, Croats, Austrians, Hungarians and Albanians around Belgrade one day soon']\n",
            " [b'\"\\n Wikipedia is just the latest casualty to \"\"O\\'Sullivan\\'s Law\"\"; and the double-standards and oily patrician-like attitudes of some admins are blatant. I expect the whole place to be thoroughly dhimmi within a year  what else is possible from a \"\"tragedy of the commons\"\" train-wreck built on a premise that a jihadist jackasshole stands on equal-footing to a studious researcher? The \"\"fix\"\" is already in with \"\"anonymous editor\"\" and his Islamist buddies Yuber & Co.  \"']\n",
            " [b'American Airlines \\n\\nSorry that was joke. I mess up after many of beverages.']\n",
            " [b'\"\\n\\n Question \\n\\nI have uploaded the correct logo under the \"\"logo\"\" linces and im wondering if you can check things over please? please reply to my tlk page\\n\\n \"']\n",
            " [b'\", darn, darn.  keep this here and OFF my page or i\\'ll file another complaint.  I\"\"M DONE!!!!  HE GONE!!!!  KA-BOOOOOOOM!!!! \"']\n",
            " [b\"Baka, you're skating on perilously thin ice in terms of civility here. You dont want to go through it, I'm sure, so get a hold of yourself. About policy, since you seem to not have found it in spite o the fact that its liberally quoted across the discussion, here it is:     \\n The subject publicly self-identifies with the belief or preference in question\\n The subject's beliefs or sexual preferences are relevant to the subject's notable activities or public life.\\nFor further reference: \\n\\nHave fun. And please, tone down the antagonism. I know I should expect no better from you, but someone else might.\"]\n",
            " [b'fuck you \\n\\nyoufuckingidiot']\n",
            " [b'WIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUNISM IS CENSORSHIP. WIKIPEDIA IS COMMUNISM.\\nWIKIPEDIA LOVES ME. THEY LIKE TO CENSOR ME. COMMUN']\n",
            " [b\"Editing talk pages \\n\\nIn the nicest and politest way possible, I would appreciate it if you did not edit my talk page and I wont edit yours. \\nIf you don't want me here then ban my IP address otherwise, please stop reverting your changes on my talkpage.\\nThanks.\"]\n",
            " [b\"Comment, and it's not just about the language comming later. It's about the language being named after something that was named after the island. And just a brief disclaimer, I'm a Java programmer.\"]\n",
            " [b'\"\\n\\n Restricted biotech a characeristic of nanopunk?! \\n\\nThe 2nd sentence in the article reads the following:\\n\"\"The genre is similar to biopunk, but describes the world where the use of biotechnologies are limited or prohibited, so only nanites and nanotechnologies are widely use (while in biopunk, bio- and nanotechnologies often coexist)\"\"\\nI\\'m really doubting the truth content of it. Basically I don\\'t think that it\\'s a characteristic of nanopunk that biotech is limited or prohibited even if some novels feature that. Additionally I\\'d rather say that this is a characteristic of biopunk with which a contrast is seeked here. Also \"\"so only nanites and nanotechnologies are widely in use\"\" is a non sequitur - it\\'s not a logical conclusion that \"\"only nanites\"\" and nanotechnologies are widely in use because biotech is limited. Furthermore what\\'s the \"\"only\"\" referring to? Only in which area of life - medical? social? every?! Also I don\\'t think it\\'s not impossible for biotech to coexist in nanotech even though it probably (and eventually that might be a core characteristic) plays a subordinate role.\\nSo I basically think everything about that sentence is false. I might be wrong about it (parts of it). So please post whatever you think of that here.\\nIf there are no objections I\\'m gonna delete that sentence and somehow get the \"\"in nanopunk biotech plays a subordinate role to nanotechnology\"\"-part in there (which isn\\'t really what the quoted sentence conveys).   \"']\n",
            " [b'You mother fucker!126.12.157.168']], shape=(30, 1), dtype=string)\n",
            "A batch of targets tf.Tensor([0 1 0 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 0 0 1], shape=(30,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GET POINTERS TO PRE-TRAINED LaBSE"
      ],
      "metadata": {
        "id": "fNf0fz3hJFrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pretrained LaBSE and preprocessor\n",
        "\n",
        "# Using LaBSE 2\n",
        "\n",
        "tfhub_handle_encoder = \"https://tfhub.dev/google/LaBSE/2\"\n",
        "\n",
        "# Will need this for pre-processing the text. To get the tensors\n",
        "\n",
        "tfhub_handle_preprocess = \"https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-preprocess/2\"\n"
      ],
      "metadata": {
        "id": "xmBut87pYwP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CREATE A FUNCTION SO CAN SAVE SETTINGS AND RESULTS HISTORY"
      ],
      "metadata": {
        "id": "WkvwQ7XQGv8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to add a record to a json file\n",
        "\n",
        "def append_record(record):\n",
        "    with open('/content/gdrive/MyDrive/266_datasets/settings/settings_history.txt', 'a') as f:\n",
        "        json.dump(record, f)\n",
        "        f.write(os.linesep)"
      ],
      "metadata": {
        "id": "x3dTY_Tubru7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BUILD LaBSE ONLY MODEL"
      ],
      "metadata": {
        "id": "taqmQ0C6JLyu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CREATE INSTANCE OF LaBSE ONLY MODEL FOR KOREAN DATA"
      ],
      "metadata": {
        "id": "zLDOAzu5S2fh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use for F1 metrics on Korean data\n",
        "statefull_binary_fbeta_korean_lab = StatefullBinaryFBeta() \n",
        "statefull_multi_class_fbeta_korean_lab = StatefullMultiClassFBeta()\n",
        "\n",
        "statefull_multi_class_fbeta_korean_lab_weighted = StatefullMultiClassFBetaWeighted()"
      ],
      "metadata": {
        "id": "VUeG4KSpzDqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use this method to build the model with just LaBSE\n",
        "\n",
        "def build_lab(lr, first_layer, second_layer, first_drop, second_drop, third_drop, first_metric, second_metric, third_metric):\n",
        " text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name= 'comments')\n",
        " preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name= 'preprocessing')\n",
        " encoder_inputs = preprocessing_layer(text_input)\n",
        " encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=False, name='LaBSE_encoder')\n",
        "\n",
        " # This will get all the outputs as a dictionary\n",
        " outputs = encoder(encoder_inputs)\n",
        "\n",
        " # pooled output represents the entire example passed.\n",
        " net = outputs['pooled_output']\n",
        "\n",
        " # Normalization. tf.nn.l2_normalize is alias for tf.math.l2_normalize. Not needed for our instance\n",
        " # normalized_sentence_representation = tf.nn.l2_normalize(net, axis=-1)\n",
        "\n",
        " # first dropout layer \n",
        " net = tf.keras.layers.Dropout(first_drop)(net)\n",
        "\n",
        " # a dense layer and relu\n",
        " net = tf.keras.layers.Dense(first_layer, activation='relu', kernel_initializer='he_uniform')(net)\n",
        " # normalization\n",
        " net = tf.keras.layers.LayerNormalization()(net)\n",
        " # second dropout layer \n",
        " net = tf.keras.layers.Dropout(second_drop)(net)\n",
        "\n",
        " # a dense layer and relu\n",
        " net = tf.keras.layers.Dense(second_layer, activation='relu')(net)\n",
        " # normalization\n",
        " net = tf.keras.layers.LayerNormalization()(net)\n",
        " #  third dropout layer \n",
        " net = tf.keras.layers.Dropout(third_drop)(net)\n",
        "\n",
        " # sigmoid\n",
        " net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(net)\n",
        "\n",
        " \n",
        " labse_model = tf.keras.Model(text_input, net)\n",
        "\n",
        " labse_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "                    optimizer=tf.keras.optimizers.Adam(lr),\n",
        "                     metrics= [first_metric, second_metric, third_metric])\n",
        " \n",
        " # Save the hyperparameters which were used\n",
        " settings = {'lr':lr,\n",
        "              'first_layer_neurons': first_layer,\n",
        "              'second_layer_neurons': second_layer,\n",
        "              'first_drop': first_drop,\n",
        "              'second_drop': second_drop,\n",
        "              'third drop': third_drop,\n",
        "              }\n",
        " return labse_model, settings\n"
      ],
      "metadata": {
        "id": "W8HuwCePFBay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  CALLBACK FOR EARLY STOP"
      ],
      "metadata": {
        "id": "T_HO3xaozhqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_state_full_binary_fbeta', mode=\"max\", patience=3)"
      ],
      "metadata": {
        "id": "fUezDQCqpxar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LaBSE ONLY KOREAN MODEL INSTANTIATE"
      ],
      "metadata": {
        "id": "OXrOKY4_zKSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Return model from function build_lab_cnn\n",
        "# Use the following learning rate, dense layer neu, dense layer neur, drop-out rate, drop-out rate two, drop-out rate three\n",
        "#args = (.001, 200, 128,  0.1, 0.2, 0) # These settings work better\n",
        "args = (.001, 256, 128,  0, 0, 0) # These values are like the control mBERT settings.\n",
        "\n",
        "# Return the following metrics\n",
        "kwargs = {\"first_metric\": statefull_binary_fbeta_korean_lab, \"second_metric\": statefull_multi_class_fbeta_korean_lab , \"third_metric\": statefull_multi_class_fbeta_korean_lab_weighted}\n",
        "\n",
        "# Return the model and the settings used in model\n",
        "model_korean, settings = build_lab(*args, **kwargs)"
      ],
      "metadata": {
        "id": "tPTpHJ4tHnsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FIT  LaBSE ONLY MODEL FOR KOREAN TRAIN DATA"
      ],
      "metadata": {
        "id": "PtCUMM47Tbu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_korean.reset_states()"
      ],
      "metadata": {
        "id": "tQi1qVPdQ4-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the LaBSE model\n",
        "\n",
        "\n",
        "history_korean_lab = model_korean.fit(x=train_ds,validation_data=val_ds,\n",
        "                               epochs=15, batch_size=30, callbacks=[callback])\n",
        "\n",
        "append_record({'language':\"korean\", 'setings':settings, 'history':history_korean_lab.history})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEAD0rZq6iNr",
        "outputId": "3d0665cc-84a6-425e-80e4-70c83073d28f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "195/195 [==============================] - ETA: 0s - loss: 0.6005 - state_full_binary_fbeta: 0.6818 - state_full_binary_fbeta_macro: 0.6643 - state_full_binary_fbeta_weighted: 0.6643"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2034: UserWarning: Metric StatefullBinaryFBeta implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  m.reset_state()\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2034: UserWarning: Metric StatefullMultiClassFBeta implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  m.reset_state()\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2034: UserWarning: Metric StatefullMultiClassFBetaWeighted implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  m.reset_state()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r195/195 [==============================] - 61s 241ms/step - loss: 0.6005 - state_full_binary_fbeta: 0.6818 - state_full_binary_fbeta_macro: 0.6643 - state_full_binary_fbeta_weighted: 0.6643 - val_loss: 0.5787 - val_state_full_binary_fbeta: 0.7179 - val_state_full_binary_fbeta_macro: 0.6900 - val_state_full_binary_fbeta_weighted: 0.6900\n",
            "Epoch 2/15\n",
            "195/195 [==============================] - 47s 239ms/step - loss: 0.4840 - state_full_binary_fbeta: 0.7629 - state_full_binary_fbeta_macro: 0.6643 - state_full_binary_fbeta_weighted: 0.6643 - val_loss: 0.5890 - val_state_full_binary_fbeta: 0.7280 - val_state_full_binary_fbeta_macro: 0.6900 - val_state_full_binary_fbeta_weighted: 0.6900\n",
            "Epoch 3/15\n",
            "195/195 [==============================] - 46s 236ms/step - loss: 0.4033 - state_full_binary_fbeta: 0.8150 - state_full_binary_fbeta_macro: 0.6643 - state_full_binary_fbeta_weighted: 0.6643 - val_loss: 0.6148 - val_state_full_binary_fbeta: 0.7226 - val_state_full_binary_fbeta_macro: 0.6900 - val_state_full_binary_fbeta_weighted: 0.6900\n",
            "Epoch 4/15\n",
            "195/195 [==============================] - 46s 237ms/step - loss: 0.3131 - state_full_binary_fbeta: 0.8664 - state_full_binary_fbeta_macro: 0.6643 - state_full_binary_fbeta_weighted: 0.6643 - val_loss: 0.6487 - val_state_full_binary_fbeta: 0.7113 - val_state_full_binary_fbeta_macro: 0.6900 - val_state_full_binary_fbeta_weighted: 0.6900\n",
            "Epoch 5/15\n",
            "195/195 [==============================] - 46s 236ms/step - loss: 0.2007 - state_full_binary_fbeta: 0.9231 - state_full_binary_fbeta_macro: 0.6643 - state_full_binary_fbeta_weighted: 0.6643 - val_loss: 0.7642 - val_state_full_binary_fbeta: 0.7060 - val_state_full_binary_fbeta_macro: 0.6900 - val_state_full_binary_fbeta_weighted: 0.6900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MAKE SURE NO OVERFITTING ON KOREAN DATA"
      ],
      "metadata": {
        "id": "C0nx8M-SPr7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's look at a few values to make sure there was no overfitting on the train data\n",
        "korean_lab_preds = model_korean.predict(train_ds, batch_size = 10)\n"
      ],
      "metadata": {
        "id": "uYSfya5FYgl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "korean_lab_preds_df = pd.DataFrame(korean_lab_preds, columns=['predicted_train_vals'])"
      ],
      "metadata": {
        "id": "8bWHTXDogaJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(korean_lab_preds_df.head(20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xplYJUlWg1Ms",
        "outputId": "981bc4bb-f0b7-4047-990f-6ba1b79835dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    predicted_train_vals\n",
            "0               0.018118\n",
            "1               0.082727\n",
            "2               0.871191\n",
            "3               0.986051\n",
            "4               0.917859\n",
            "5               0.008914\n",
            "6               0.002901\n",
            "7               0.949750\n",
            "8               0.999072\n",
            "9               0.977956\n",
            "10              0.953331\n",
            "11              0.976660\n",
            "12              0.517538\n",
            "13              0.981117\n",
            "14              0.016906\n",
            "15              0.137702\n",
            "16              0.061911\n",
            "17              0.999131\n",
            "18              0.004190\n",
            "19              0.004186\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EVAL LaBSE ONLY MODEL ON KOREAN TEST DATA"
      ],
      "metadata": {
        "id": "_-rqntkuUkYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Eval on korean test data.\n",
        "history_korean_lab_test = model_korean.evaluate(\n",
        "                                                x=test_ds,\n",
        "                                                batch_size=None,\n",
        "                                                verbose=1,\n",
        "                                                sample_weight=None,\n",
        "                                                steps=None,\n",
        "                                                callbacks=None,\n",
        "                                                max_queue_size=10,\n",
        "                                                workers=1,\n",
        "                                                use_multiprocessing=False,\n",
        "                                                return_dict=False,\n",
        "\n",
        "                                                )\n",
        "\n",
        "append_record({'language':\"korean_test\", 'history':history_korean_lab_test})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGfpcvsi9jox",
        "outputId": "df964fa1-5d6b-4905-d7a3-713b13a9e82c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2034: UserWarning: Metric StatefullBinaryFBeta implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  m.reset_state()\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2034: UserWarning: Metric StatefullMultiClassFBeta implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  m.reset_state()\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2034: UserWarning: Metric StatefullMultiClassFBetaWeighted implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  m.reset_state()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25/25 [==============================] - 6s 230ms/step - loss: 0.8100 - state_full_binary_fbeta: 0.6878 - state_full_binary_fbeta_macro: 0.6618 - state_full_binary_fbeta_weighted: 0.6618\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CREATE A FILE FOR KOREAN TEST DATA THAT HAS ALL THE PREDICTIONS"
      ],
      "metadata": {
        "id": "4cq4pVnsSNQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "korean_lab_preds_test = model_korean.predict(test_ds, batch_size = 10)\n",
        "korean_lab_preds_test_df = pd.DataFrame(korean_lab_preds_test, columns=['predicted_test_vals'])\n",
        "\n",
        "korean_lab_preds_test_df.to_csv('/content/gdrive/My Drive/266_datasets/standard_korean_data/korean_lab_predictions_two.csv')"
      ],
      "metadata": {
        "id": "xPy8vEmijq5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FIT A LaBSE ONLY MODEL WITH HINDI"
      ],
      "metadata": {
        "id": "75NFxLOm0R45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom F1 metric\n",
        "\n",
        "statefull_binary_fbeta_hindi_lab = StatefullBinaryFBeta() \n",
        "\n",
        "statefull_multi_class_fbeta_hindi_lab = StatefullMultiClassFBeta()\n",
        "\n",
        "statefull_multi_class_fbeta_hindi_lab_weighted = StatefullMultiClassFBetaWeighted()"
      ],
      "metadata": {
        "id": "JU3lymgG0XzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a LaBSE only model with just Hindi train data\n",
        "\n",
        "\n",
        "# Return model from function build_lab\n",
        "#args = (.001, 256, 128,  0, 0, 0)\n",
        "args = (.001, 200, 128,  0.1, 0.2, 0)\n",
        "\n",
        "kwargs = {\"first_metric\": statefull_binary_fbeta_hindi_lab, \"second_metric\": statefull_multi_class_fbeta_hindi_lab , \"third_metric\": statefull_multi_class_fbeta_hindi_lab_weighted}\n",
        "model_hindi, settings = build_lab(*args, **kwargs)"
      ],
      "metadata": {
        "id": "vs3cM9Ee0tTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_hindi.reset_states()"
      ],
      "metadata": {
        "id": "cbl00OCNRXEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit LaBSE only model for Hindi \n",
        "\n",
        "history_hindi_lab = model_hindi.fit(x=train_ds_hindi,validation_data=val_ds_hindi,\n",
        "                               epochs=15, batch_size=30, callbacks=[callback])\n",
        "\n",
        "append_record({'language':\"hindi\", 'setings':settings, 'history':history_hindi_lab.history})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apeunn5W00y0",
        "outputId": "bace3127-c5dc-431a-fb20-5b61c9dbc237"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "141/142 [============================>.] - ETA: 0s - loss: 0.6261 - state_full_binary_fbeta: 0.6567 - state_full_binary_fbeta_macro: 0.6631 - state_full_binary_fbeta_weighted: 0.6631"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2034: UserWarning: Metric StatefullBinaryFBeta implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  m.reset_state()\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2034: UserWarning: Metric StatefullMultiClassFBeta implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  m.reset_state()\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2034: UserWarning: Metric StatefullMultiClassFBetaWeighted implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  m.reset_state()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r142/142 [==============================] - 46s 246ms/step - loss: 0.6257 - state_full_binary_fbeta: 0.6569 - state_full_binary_fbeta_macro: 0.6630 - state_full_binary_fbeta_weighted: 0.6630 - val_loss: 0.5901 - val_state_full_binary_fbeta: 0.7231 - val_state_full_binary_fbeta_macro: 0.6792 - val_state_full_binary_fbeta_weighted: 0.6792\n",
            "Epoch 2/15\n",
            "142/142 [==============================] - 34s 240ms/step - loss: 0.5447 - state_full_binary_fbeta: 0.7237 - state_full_binary_fbeta_macro: 0.6630 - state_full_binary_fbeta_weighted: 0.6630 - val_loss: 0.5763 - val_state_full_binary_fbeta: 0.7110 - val_state_full_binary_fbeta_macro: 0.6792 - val_state_full_binary_fbeta_weighted: 0.6792\n",
            "Epoch 3/15\n",
            "142/142 [==============================] - 34s 240ms/step - loss: 0.5107 - state_full_binary_fbeta: 0.7407 - state_full_binary_fbeta_macro: 0.6630 - state_full_binary_fbeta_weighted: 0.6630 - val_loss: 0.6013 - val_state_full_binary_fbeta: 0.6640 - val_state_full_binary_fbeta_macro: 0.6792 - val_state_full_binary_fbeta_weighted: 0.6792\n",
            "Epoch 4/15\n",
            "142/142 [==============================] - 34s 239ms/step - loss: 0.4702 - state_full_binary_fbeta: 0.7649 - state_full_binary_fbeta_macro: 0.6630 - state_full_binary_fbeta_weighted: 0.6630 - val_loss: 0.6190 - val_state_full_binary_fbeta: 0.6780 - val_state_full_binary_fbeta_macro: 0.6792 - val_state_full_binary_fbeta_weighted: 0.6792\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Eval on hindi test data\n",
        "history_hindi_lab_test = model_hindi.evaluate(\n",
        "                                              x=test_ds_hindi,\n",
        "                                              batch_size=None,\n",
        "                                              verbose=1,\n",
        "                                              sample_weight=None,\n",
        "                                              steps=None,\n",
        "                                              callbacks=None,\n",
        "                                              max_queue_size=10,\n",
        "                                              workers=1,\n",
        "                                              use_multiprocessing=False,\n",
        "                                              return_dict=False,\n",
        "\n",
        "                                              )\n",
        "\n",
        "append_record({'language':\"hindi_test\", 'history':history_hindi_lab_test})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9snffZeM08m_",
        "outputId": "cf4c1c8a-3cc1-4e4a-99f8-56e36ab69a23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2034: UserWarning: Metric StatefullBinaryFBeta implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  m.reset_state()\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2034: UserWarning: Metric StatefullMultiClassFBeta implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  m.reset_state()\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2034: UserWarning: Metric StatefullMultiClassFBetaWeighted implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  m.reset_state()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18/18 [==============================] - 4s 210ms/step - loss: 0.5553 - state_full_binary_fbeta: 0.7218 - state_full_binary_fbeta_macro: 0.6832 - state_full_binary_fbeta_weighted: 0.6832\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hindi_lab_preds_test = model_hindi.predict(test_ds_hindi, batch_size = 10)\n",
        "hindi_lab_preds_test_df = pd.DataFrame(hindi_lab_preds_test, columns=['predicted_test_vals'])\n",
        "\n",
        "hindi_lab_preds_test_df.to_csv('/content/gdrive/My Drive/266_datasets/standard_hindi_data/hindi_lab_predictions_two.csv')"
      ],
      "metadata": {
        "id": "xy037_br1Dzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DO ZERO SHOT LEARNING WITH LaBSE ONLY MODEL USING ENGLISH TO TRAIN AND KOREAN TO EVAL"
      ],
      "metadata": {
        "id": "kgsITtgzSkDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use for F1 metrics on English data\n",
        "statefull_binary_fbeta_english_lab = StatefullBinaryFBeta() \n",
        "statefull_multi_class_fbeta_english_lab = StatefullMultiClassFBeta()\n",
        "\n",
        "statefull_multi_class_fbeta_english_lab_weighted = StatefullMultiClassFBetaWeighted()"
      ],
      "metadata": {
        "id": "MiRD8vnZciaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Return model from function build_lab_cnn\n",
        "# Use the following learning rate, dense layer neu, dense layer neur, drop-out rate, drop-out rate two, drop-out rate three\n",
        "args = (.001, 200, 128,  0.1, 0.2, 0) # These settings work better\n",
        "#args = (.001, 256, 128,  0, 0, 0) # These values are like the control mBERT settings.\n",
        "\n",
        "# Return the following metrics\n",
        "kwargs = {\"first_metric\": statefull_binary_fbeta_english_lab, \"second_metric\": statefull_multi_class_fbeta_english_lab , \"third_metric\": statefull_multi_class_fbeta_english_lab_weighted}\n",
        "\n",
        "# Return the model and the settings used in model\n",
        "model_english, settings = build_lab(*args, **kwargs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "rMP05VITcdPg",
        "outputId": "b2a853b6-a7a4-410a-82ce-f4f12a00b4c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-112-04ce49f27e03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Return the model and the settings used in model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel_english\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msettings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_lab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-625c9def8844>\u001b[0m in \u001b[0;36mbuild_lab\u001b[0;34m(lr, first_layer, second_layer, first_drop, second_drop, third_drop, first_metric, second_metric, third_metric)\u001b[0m\n\u001b[1;32m      5\u001b[0m  \u001b[0mpreprocessing_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfhub_handle_preprocess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'preprocessing'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m  \u001b[0mencoder_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m  \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfhub_handle_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'LaBSE_encoder'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m  \u001b[0;31m# This will get all the outputs as a dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, handle, trainable, arguments, _sentinel, tags, signature, signature_outputs_as_dict, output_key, output_shape, load_options, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_training_argument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_has_training_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_hub_module_v1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_is_hub_module_v1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36mload_module\u001b[0;34m(handle, tags, load_options)\u001b[0m\n\u001b[1;32m    447\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Expected before TF2.4.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0mset_load_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodule_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset_load_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_hub/module_v2.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(handle, tags, options)\u001b[0m\n\u001b[1;32m    104\u001b[0m         module_path, tags=tags, options=options)\n\u001b[1;32m    105\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m   \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_hub_module_v1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_hub_module_v1\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(export_dir, tags, options)\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mdon\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0ma\u001b[0m \u001b[0mMetaGraph\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mSavedModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m   \"\"\"\n\u001b[0;32m--> 936\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"root\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload_internal\u001b[0;34m(export_dir, tags, options, loader_cls, filters)\u001b[0m\n\u001b[1;32m    973\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m         loader = loader_cls(object_graph_proto, saved_model_proto, export_dir,\n\u001b[0;32m--> 975\u001b[0;31m                             ckpt_options, options, filters)\n\u001b[0m\u001b[1;32m    976\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m         raise FileNotFoundError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, object_graph_proto, saved_model_proto, export_dir, ckpt_options, save_options, filters)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mlibrary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeta_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibrary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0msaved_object_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_proto\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m             wrapper_function=_WrapperFunction))\n\u001b[0m\u001b[1;32m    153\u001b[0m     \u001b[0;31m# Store a set of all concrete functions that have been set up with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;31m# captures.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/function_deserialization.py\u001b[0m in \u001b[0;36mload_function_def_library\u001b[0;34m(library, saved_object_graph, load_shared_name_suffix, wrapper_function)\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0;31m# Restores gradients for function-call ops (not the same as ops that use\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[0;31m# custom gradients)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m     \u001b[0m_restore_gradient_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenamed_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaded_gradients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfunction_deps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfdef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/function_deserialization.py\u001b[0m in \u001b[0;36m_restore_gradient_functions\u001b[0;34m(func_graph, renamed_functions, loaded_gradients)\u001b[0m\n\u001b[1;32m    474\u001b[0m       \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_gradient_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m       \u001b[0mgradient_op_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_gradient_op_type\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2688\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2689\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2690\u001b[0;31m         \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_OperationGetAttrValueProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2691\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FIT A MODEL ON ENGLISH DATA"
      ],
      "metadata": {
        "id": "OoiVFv7wTCA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_english.reset_states()\n"
      ],
      "metadata": {
        "id": "_3NG1XsYRxe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the LaBSE model\n",
        "\n",
        "\n",
        "history_english_lab = model_english.fit(x=train_ds_english,validation_data=val_ds_english,\n",
        "                               epochs=15, batch_size=30, callbacks=[callback])\n",
        "\n",
        "append_record({'language':\"english\", 'setings':settings, 'history':history_english_lab.history})"
      ],
      "metadata": {
        "id": "Q5qDTE-iTHbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EVALUATE KOREAN TEST DATA ON ENGLISH TRAINED LaBSE"
      ],
      "metadata": {
        "id": "KIfxowlJak22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Zero- shot. Trained on English and eval on korean test data.\n",
        "history_eng_korean_lab_test = model_english.evaluate(\n",
        "                                                x=test_ds,\n",
        "                                                batch_size=None,\n",
        "                                                verbose=1,\n",
        "                                                sample_weight=None,\n",
        "                                                steps=None,\n",
        "                                                callbacks=None,\n",
        "                                                max_queue_size=10,\n",
        "                                                workers=1,\n",
        "                                                use_multiprocessing=False,\n",
        "                                                return_dict=False,\n",
        "\n",
        "                                                )\n",
        "\n",
        "append_record({'language':\"engligh_korean_test\", 'history':history_eng_korean_lab_test})"
      ],
      "metadata": {
        "id": "C9oSBkH5atnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PREDICT ON KOREAN TEST DATA\n",
        "\n",
        "korean_eng_lab_preds_test = model_english.predict(test_ds, batch_size = 10)\n",
        "korean_eng_lab_preds_test_df = pd.DataFrame(korean_eng_lab_preds_test, columns=['predicted_test_vals'])\n",
        "\n",
        "korean_eng_lab_preds_test_df.to_csv('/content/gdrive/My Drive/266_datasets/standard_english_data/korean_eng_lab_predictions_two.csv')\n"
      ],
      "metadata": {
        "id": "vXFb5aofjhQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EVALUATE HINDI TEST ON ENGLISH TRAINED LaBSE"
      ],
      "metadata": {
        "id": "DwnPeLLTk1wu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Zero- hot. Trained on English and eval on hindi test data.\n",
        "history_eng_hindi_lab_test = model_english.evaluate(\n",
        "                                                x=test_ds_hindi,\n",
        "                                                batch_size=None,\n",
        "                                                verbose=1,\n",
        "                                                sample_weight=None,\n",
        "                                                steps=None,\n",
        "                                                callbacks=None,\n",
        "                                                max_queue_size=10,\n",
        "                                                workers=1,\n",
        "                                                use_multiprocessing=False,\n",
        "                                                return_dict=False,\n",
        "\n",
        "                                                )\n",
        "\n",
        "append_record({'language':\"english_hindi_test\", 'history':history_eng_hindi_lab_test})"
      ],
      "metadata": {
        "id": "0X96op1vk8tE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on Hindi test data\n",
        "hindi_eng_lab_preds_test = model_english.predict(test_ds_hindi, batch_size = 10)\n",
        "hindi_eng_lab_preds_test_df = pd.DataFrame(hindi_eng_lab_preds_test, columns=['predicted_test_vals'])\n",
        "\n",
        "hindi_eng_lab_preds_test_df.to_csv('/content/gdrive/My Drive/266_datasets/standard_english_data/hindi_eng_lab_predictions_two.csv')"
      ],
      "metadata": {
        "id": "guq1PymxmsRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_df_hindi.head(45))"
      ],
      "metadata": {
        "id": "dm8TaICanQl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DEFINE A MODEL WITH LaBSE AND CNN"
      ],
      "metadata": {
        "id": "8n_EKFhnISuh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## USE CUSTOM F1 METRIC CLASS ON LaBSE WITH CNN KOREAN MODEL"
      ],
      "metadata": {
        "id": "gNcPKQA-5-B4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "statefull_binary_fbeta_korean_lab_cnn = StatefullBinaryFBeta() \n",
        "statefull_multi_class_fbeta_korean_lab_cnn = StatefullMultiClassFBeta()\n",
        "\n",
        "statefull_multi_class_fbeta_korean_lab_cnn_weighted = StatefullMultiClassFBetaWeighted()\n"
      ],
      "metadata": {
        "id": "7smSkOEqDnpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using a model with LaBSE with CNN on top instead\n",
        "\n",
        "#num_filters = [32, 32, 32, 32, 32]\n",
        "#kernel_sizes = [1, 2, 3, 4,5]\n",
        "#dense_layer_dims = [10, 4]\n",
        "\n",
        "#num_classes = len(ds.target_names)\n",
        "\n",
        "def build_lab_cnn(lr, first_layer, second_layer, first_drop, num_filters, kernel_sizes, first_metric, second_metric, third_metric):\n",
        " text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name= 'comments')\n",
        " preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name= 'preprocessing')\n",
        " encoder_inputs = preprocessing_layer(text_input)\n",
        " encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=False, name='LaBSE_encoder')\n",
        "\n",
        " # This will get all the outputs as a dictionary\n",
        " outputs = encoder(encoder_inputs)\n",
        "\n",
        " \n",
        " # sequence output is contextual embedding of each token\n",
        " net = outputs['sequence_output']\n",
        "\n",
        " conv_layers_for_all_kernel_sizes = []\n",
        " for kernel_size, filters in zip(kernel_sizes, num_filters):\n",
        "    conv_layer = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, activation='relu')(net)\n",
        "    conv_layer = keras.layers.GlobalMaxPooling1D()(conv_layer)\n",
        "    conv_layers_for_all_kernel_sizes.append(conv_layer)\n",
        "\n",
        " # Concat the feature maps from each different size. Flattening\n",
        " h = keras.layers.concatenate(conv_layers_for_all_kernel_sizes, axis=1)\n",
        "\n",
        " h = keras.layers.Dropout(rate=first_drop)(h)\n",
        " \n",
        " prediction = keras.layers.Dense(1, activation='sigmoid')(h)\n",
        "\n",
        "\n",
        " labse_cnn_model = tf.keras.Model(inputs=text_input, outputs=prediction)\n",
        "\n",
        " # Compile the model\n",
        " # Logit set to false because already have sigmoid in model design\n",
        "\n",
        " labse_cnn_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "                          optimizer=tf.keras.optimizers.Adam(lr=lr),\n",
        "                          metrics=[first_metric, second_metric, third_metric])\n",
        "\n",
        " settings_lab_cnn = {'lr':lr,\n",
        "                     'first_layer_neurons': first_layer,\n",
        "                     'second_layer_neurons': second_layer,\n",
        "                     'first_drop': first_drop,\n",
        "                     'num_filters': num_filters,\n",
        "                     'kernel_sizes': kernel_sizes\n",
        "                    }\n",
        "\n",
        "\n",
        " return labse_cnn_model, settings_lab_cnn\n"
      ],
      "metadata": {
        "id": "uUJh-g8a4V-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CREATE INSTANCE OF LaBSE WITH CNN MODEL FOR KOREAN DATA"
      ],
      "metadata": {
        "id": "GSYMrBEz5xi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Return model from function build_lab_cnn\n",
        "# Use the following learning rate, dense layer neu, dense layer neur, drop-out rate\n",
        "#args = (.001, 200, 128,  0.1, 0.2, 0) # These settings work better\n",
        "#args = (.001, 256, 128,  0, 0, 0) # These values are like the control mBERT settings.\n",
        "num_filters = [32, 32, 32, 32, 32]\n",
        "kernel_sizes = [1, 2, 3, 4,5]\n",
        "args = (0.00002, 0,0,0.7)\n",
        "\n",
        "# Return the following metrics\n",
        "kwargs = {\"num_filters\": num_filters, \"kernel_sizes\": kernel_sizes,\"first_metric\": statefull_binary_fbeta_korean_lab_cnn, \"second_metric\": statefull_multi_class_fbeta_korean_lab_cnn , \"third_metric\": statefull_multi_class_fbeta_korean_lab_cnn_weighted}\n",
        "\n",
        "# Return the model and the settings used in model\n",
        "model_korean_two, settings_lab_cnn = build_lab_cnn(*args, **kwargs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMB4APeicYCL",
        "outputId": "aaab3576-dbb2-4be5-aece-53f69870c59f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_korean_two.summary()"
      ],
      "metadata": {
        "id": "tDzaiWd2UCgm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "479dc0f8-a66b-486b-d4c7-ebcb81db7c72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " comments (InputLayer)          [(None,)]            0           []                               \n",
            "                                                                                                  \n",
            " preprocessing (KerasLayer)     {'input_type_ids':   0           ['comments[0][0]']               \n",
            "                                (None, 128),                                                      \n",
            "                                 'input_mask': (Non                                               \n",
            "                                e, 128),                                                          \n",
            "                                 'input_word_ids':                                                \n",
            "                                (None, 128)}                                                      \n",
            "                                                                                                  \n",
            " LaBSE_encoder (KerasLayer)     {'sequence_output':  470926849   ['preprocessing[0][0]',          \n",
            "                                 (None, 128, 768),                'preprocessing[0][1]',          \n",
            "                                 'encoder_outputs':               'preprocessing[0][2]']          \n",
            "                                 [(None, 128, 768),                                               \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768)],                                               \n",
            "                                 'default': (None,                                                \n",
            "                                768),                                                             \n",
            "                                 'pooled_output': (                                               \n",
            "                                None, 768)}                                                       \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 128, 32)      24608       ['LaBSE_encoder[0][14]']         \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 127, 32)      49184       ['LaBSE_encoder[0][14]']         \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 126, 32)      73760       ['LaBSE_encoder[0][14]']         \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 125, 32)      98336       ['LaBSE_encoder[0][14]']         \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 124, 32)      122912      ['LaBSE_encoder[0][14]']         \n",
            "                                                                                                  \n",
            " global_max_pooling1d (GlobalMa  (None, 32)          0           ['conv1d[0][0]']                 \n",
            " xPooling1D)                                                                                      \n",
            "                                                                                                  \n",
            " global_max_pooling1d_1 (Global  (None, 32)          0           ['conv1d_1[0][0]']               \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " global_max_pooling1d_2 (Global  (None, 32)          0           ['conv1d_2[0][0]']               \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " global_max_pooling1d_3 (Global  (None, 32)          0           ['conv1d_3[0][0]']               \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " global_max_pooling1d_4 (Global  (None, 32)          0           ['conv1d_4[0][0]']               \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 160)          0           ['global_max_pooling1d[0][0]',   \n",
            "                                                                  'global_max_pooling1d_1[0][0]', \n",
            "                                                                  'global_max_pooling1d_2[0][0]', \n",
            "                                                                  'global_max_pooling1d_3[0][0]', \n",
            "                                                                  'global_max_pooling1d_4[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_15 (Dropout)           (None, 160)          0           ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dense_10 (Dense)               (None, 1)            161         ['dropout_15[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 471,295,810\n",
            "Trainable params: 368,961\n",
            "Non-trainable params: 470,926,849\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FIT LaBSE WITH CNN KOREAN MODEL"
      ],
      "metadata": {
        "id": "BL9VI4yPXxeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_korean_two.reset_states()\n",
        "history_lab_cnn_korean = model_korean_two.fit(x=train_ds,validation_data=val_ds,\n",
        "                               epochs=15, batch_size=30, callbacks=[callback])\n",
        "\n",
        "\n",
        "append_record({'language':\"korean\", 'setings':settings_lab_cnn, 'history':history_lab_cnn_korean.history})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9HJCY9zXX-K",
        "outputId": "80eda6e3-9af6-4c3e-8196-411471d25c41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "195/195 [==============================] - 71s 273ms/step - loss: 1.0887 - state_full_binary_fbeta: 0.5390 - state_full_binary_fbeta_macro: 0.6643 - state_full_binary_fbeta_weighted: 0.6643 - val_loss: 0.6440 - val_state_full_binary_fbeta: 0.6219 - val_state_full_binary_fbeta_macro: 0.6900 - val_state_full_binary_fbeta_weighted: 0.6900\n",
            "Epoch 2/15\n",
            "195/195 [==============================] - 56s 290ms/step - loss: 0.9001 - state_full_binary_fbeta: 0.5731 - state_full_binary_fbeta_macro: 0.6643 - state_full_binary_fbeta_weighted: 0.6643 - val_loss: 0.6030 - val_state_full_binary_fbeta: 0.6930 - val_state_full_binary_fbeta_macro: 0.6900 - val_state_full_binary_fbeta_weighted: 0.6900\n",
            "Epoch 3/15\n",
            "195/195 [==============================] - 48s 248ms/step - loss: 0.7732 - state_full_binary_fbeta: 0.6054 - state_full_binary_fbeta_macro: 0.6643 - state_full_binary_fbeta_weighted: 0.6643 - val_loss: 0.5891 - val_state_full_binary_fbeta: 0.7099 - val_state_full_binary_fbeta_macro: 0.6900 - val_state_full_binary_fbeta_weighted: 0.6900\n",
            "Epoch 4/15\n",
            "195/195 [==============================] - 49s 251ms/step - loss: 0.7167 - state_full_binary_fbeta: 0.6269 - state_full_binary_fbeta_macro: 0.6643 - state_full_binary_fbeta_weighted: 0.6643 - val_loss: 0.5851 - val_state_full_binary_fbeta: 0.7030 - val_state_full_binary_fbeta_macro: 0.6900 - val_state_full_binary_fbeta_weighted: 0.6900\n",
            "Epoch 5/15\n",
            "195/195 [==============================] - 48s 248ms/step - loss: 0.6606 - state_full_binary_fbeta: 0.6571 - state_full_binary_fbeta_macro: 0.6643 - state_full_binary_fbeta_weighted: 0.6643 - val_loss: 0.5798 - val_state_full_binary_fbeta: 0.7082 - val_state_full_binary_fbeta_macro: 0.6900 - val_state_full_binary_fbeta_weighted: 0.6900\n",
            "Epoch 6/15\n",
            "195/195 [==============================] - 48s 247ms/step - loss: 0.6492 - state_full_binary_fbeta: 0.6597 - state_full_binary_fbeta_macro: 0.6643 - state_full_binary_fbeta_weighted: 0.6643 - val_loss: 0.5766 - val_state_full_binary_fbeta: 0.7173 - val_state_full_binary_fbeta_macro: 0.6900 - val_state_full_binary_fbeta_weighted: 0.6900\n",
            "Epoch 7/15\n",
            "195/195 [==============================] - 48s 247ms/step - loss: 0.6195 - state_full_binary_fbeta: 0.6754 - state_full_binary_fbeta_macro: 0.6643 - state_full_binary_fbeta_weighted: 0.6643 - val_loss: 0.5708 - val_state_full_binary_fbeta: 0.7216 - val_state_full_binary_fbeta_macro: 0.6900 - val_state_full_binary_fbeta_weighted: 0.6900\n",
            "Epoch 8/15\n",
            "195/195 [==============================] - 48s 248ms/step - loss: 0.6130 - state_full_binary_fbeta: 0.6845 - state_full_binary_fbeta_macro: 0.6643 - state_full_binary_fbeta_weighted: 0.6643 - val_loss: 0.5747 - val_state_full_binary_fbeta: 0.7100 - val_state_full_binary_fbeta_macro: 0.6900 - val_state_full_binary_fbeta_weighted: 0.6900\n",
            "Epoch 9/15\n",
            "195/195 [==============================] - 48s 247ms/step - loss: 0.6073 - state_full_binary_fbeta: 0.6834 - state_full_binary_fbeta_macro: 0.6643 - state_full_binary_fbeta_weighted: 0.6643 - val_loss: 0.5705 - val_state_full_binary_fbeta: 0.7099 - val_state_full_binary_fbeta_macro: 0.6900 - val_state_full_binary_fbeta_weighted: 0.6900\n",
            "Epoch 10/15\n",
            "195/195 [==============================] - 48s 247ms/step - loss: 0.5893 - state_full_binary_fbeta: 0.6927 - state_full_binary_fbeta_macro: 0.6643 - state_full_binary_fbeta_weighted: 0.6643 - val_loss: 0.5656 - val_state_full_binary_fbeta: 0.7266 - val_state_full_binary_fbeta_macro: 0.6900 - val_state_full_binary_fbeta_weighted: 0.6900\n",
            "Epoch 11/15\n",
            "195/195 [==============================] - 48s 247ms/step - loss: 0.5790 - state_full_binary_fbeta: 0.6976 - state_full_binary_fbeta_macro: 0.6643 - state_full_binary_fbeta_weighted: 0.6643 - val_loss: 0.5643 - val_state_full_binary_fbeta: 0.7256 - val_state_full_binary_fbeta_macro: 0.6900 - val_state_full_binary_fbeta_weighted: 0.6900\n",
            "Epoch 12/15\n",
            "195/195 [==============================] - 48s 247ms/step - loss: 0.5739 - state_full_binary_fbeta: 0.7119 - state_full_binary_fbeta_macro: 0.6643 - state_full_binary_fbeta_weighted: 0.6643 - val_loss: 0.5627 - val_state_full_binary_fbeta: 0.7343 - val_state_full_binary_fbeta_macro: 0.6900 - val_state_full_binary_fbeta_weighted: 0.6900\n",
            "Epoch 13/15\n",
            "195/195 [==============================] - 48s 248ms/step - loss: 0.5670 - state_full_binary_fbeta: 0.7063 - state_full_binary_fbeta_macro: 0.6643 - state_full_binary_fbeta_weighted: 0.6643 - val_loss: 0.5597 - val_state_full_binary_fbeta: 0.7352 - val_state_full_binary_fbeta_macro: 0.6900 - val_state_full_binary_fbeta_weighted: 0.6900\n",
            "Epoch 14/15\n",
            "195/195 [==============================] - 48s 248ms/step - loss: 0.5652 - state_full_binary_fbeta: 0.7107 - state_full_binary_fbeta_macro: 0.6643 - state_full_binary_fbeta_weighted: 0.6643 - val_loss: 0.5585 - val_state_full_binary_fbeta: 0.7351 - val_state_full_binary_fbeta_macro: 0.6900 - val_state_full_binary_fbeta_weighted: 0.6900\n",
            "Epoch 15/15\n",
            "195/195 [==============================] - 48s 248ms/step - loss: 0.5472 - state_full_binary_fbeta: 0.7251 - state_full_binary_fbeta_macro: 0.6643 - state_full_binary_fbeta_weighted: 0.6643 - val_loss: 0.5592 - val_state_full_binary_fbeta: 0.7196 - val_state_full_binary_fbeta_macro: 0.6900 - val_state_full_binary_fbeta_weighted: 0.6900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EVAL KOREAN TEST DATA ON LaBSE WITH CNN MODEL "
      ],
      "metadata": {
        "id": "UxNmkCYFY3wG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Eval on korean dev data - use this if using the second way to use the input data\n",
        "history_korean_lab_cnn_test = model_korean_two.evaluate(\n",
        "    x=test_ds,\n",
        "    batch_size=None,\n",
        "    verbose=1,\n",
        "    sample_weight=None,\n",
        "    steps=None,\n",
        "    callbacks=None,\n",
        "    max_queue_size=10,\n",
        "    workers=1,\n",
        "    use_multiprocessing=False,\n",
        "    return_dict=False,\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "append_record({'language':\"korean_test\", 'history':history_korean_lab_cnn_test})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xa-irs7MYn3O",
        "outputId": "f8a27c36-de35-409e-8ef4-c7da2a763de1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25/25 [==============================] - 6s 223ms/step - loss: 0.5389 - state_full_binary_fbeta: 0.7290 - state_full_binary_fbeta_macro: 0.6618 - state_full_binary_fbeta_weighted: 0.6618\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "korean_lab_cnn_preds_test = model_korean_two.predict(test_ds, batch_size = 10)\n",
        "korean_lab_cnn_preds_test_df = pd.DataFrame(korean_lab_cnn_preds_test, columns=['predicted_test_vals'])\n",
        "\n",
        "korean_lab_cnn_preds_test_df.to_csv('/content/gdrive/My Drive/266_datasets/standard_korean_data/korean_lab_cnn_predictions.csv')"
      ],
      "metadata": {
        "id": "VMD3CRjPo6Bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BUILD A LaBSE PLUS CNN FOR HINDI"
      ],
      "metadata": {
        "id": "6xbsqDVD3sxi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## USE CUSTOM F1 METRIC CLASS ON LaBSE WITH CNN HINDI MODEL"
      ],
      "metadata": {
        "id": "TWT6medF_AZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# F1 score from custom module for hindi lab/cnn\n",
        "statefull_binary_fbeta_hindi_lab_cnn = StatefullBinaryFBeta() \n",
        "statefull_multi_class_fbeta_hindi_lab_cnn = StatefullMultiClassFBeta()\n",
        "\n",
        "statefull_multi_class_fbeta_hindi_lab_cnn_weighted = StatefullMultiClassFBetaWeighted()"
      ],
      "metadata": {
        "id": "urKfM8TFAIvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "num_filters = [32, 32, 32, 32, 32]\n",
        "kernel_sizes = [1, 2, 3, 4,5]\n",
        "args = (0.00002, 0,0,0.7)\n",
        "\n",
        "# Return the following metrics\n",
        "kwargs = {\"num_filters\": num_filters, \"kernel_sizes\": kernel_sizes,\"first_metric\": statefull_binary_fbeta_hindi_lab_cnn, \"second_metric\": statefull_multi_class_fbeta_hindi_lab_cnn , \"third_metric\": statefull_multi_class_fbeta_hindi_lab_cnn_weighted}\n",
        "\n",
        "# Return the model and the settings used in model\n",
        "model_hindi_two, settings_lab_cnn = build_lab_cnn(*args, **kwargs)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kSwYKATY3bW2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a587aab0-4773-4cff-9092-82255da08934"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FIT LaBSE WITH CNN FOR HINDI"
      ],
      "metadata": {
        "id": "e8uCPsON4LcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_hindi_two.reset_states()\n",
        "history_lab_cnn_hindi = model_hindi_two.fit(x=train_ds_hindi,validation_data=val_ds_hindi,\n",
        "                               epochs=15, batch_size=30, callbacks=[callback])\n",
        "\n",
        "\n",
        "\n",
        "append_record({'language':\"hindi\", 'setings':settings_lab_cnn, 'history':history_lab_cnn_hindi.history})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHLxfrpe4Dnp",
        "outputId": "d97cde63-99dd-4bdf-fe45-818e3763bd58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "142/142 [==============================] - 45s 259ms/step - loss: 1.6494 - state_full_binary_fbeta: 0.4986 - state_full_binary_fbeta_macro: 0.6630 - state_full_binary_fbeta_weighted: 0.6630 - val_loss: 0.7938 - val_state_full_binary_fbeta: 0.4386 - val_state_full_binary_fbeta_macro: 0.6792 - val_state_full_binary_fbeta_weighted: 0.6792\n",
            "Epoch 2/15\n",
            "142/142 [==============================] - 35s 249ms/step - loss: 1.3277 - state_full_binary_fbeta: 0.5331 - state_full_binary_fbeta_macro: 0.6630 - state_full_binary_fbeta_weighted: 0.6630 - val_loss: 0.7003 - val_state_full_binary_fbeta: 0.5939 - val_state_full_binary_fbeta_macro: 0.6792 - val_state_full_binary_fbeta_weighted: 0.6792\n",
            "Epoch 3/15\n",
            "142/142 [==============================] - 35s 248ms/step - loss: 1.1433 - state_full_binary_fbeta: 0.5588 - state_full_binary_fbeta_macro: 0.6630 - state_full_binary_fbeta_weighted: 0.6630 - val_loss: 0.6745 - val_state_full_binary_fbeta: 0.6357 - val_state_full_binary_fbeta_macro: 0.6792 - val_state_full_binary_fbeta_weighted: 0.6792\n",
            "Epoch 4/15\n",
            "142/142 [==============================] - 35s 249ms/step - loss: 0.9900 - state_full_binary_fbeta: 0.5890 - state_full_binary_fbeta_macro: 0.6630 - state_full_binary_fbeta_weighted: 0.6630 - val_loss: 0.6540 - val_state_full_binary_fbeta: 0.6573 - val_state_full_binary_fbeta_macro: 0.6792 - val_state_full_binary_fbeta_weighted: 0.6792\n",
            "Epoch 5/15\n",
            "142/142 [==============================] - 35s 248ms/step - loss: 0.9088 - state_full_binary_fbeta: 0.5963 - state_full_binary_fbeta_macro: 0.6630 - state_full_binary_fbeta_weighted: 0.6630 - val_loss: 0.6405 - val_state_full_binary_fbeta: 0.6748 - val_state_full_binary_fbeta_macro: 0.6792 - val_state_full_binary_fbeta_weighted: 0.6792\n",
            "Epoch 6/15\n",
            "142/142 [==============================] - 35s 249ms/step - loss: 0.8116 - state_full_binary_fbeta: 0.6191 - state_full_binary_fbeta_macro: 0.6630 - state_full_binary_fbeta_weighted: 0.6630 - val_loss: 0.6277 - val_state_full_binary_fbeta: 0.6517 - val_state_full_binary_fbeta_macro: 0.6792 - val_state_full_binary_fbeta_weighted: 0.6792\n",
            "Epoch 7/15\n",
            "142/142 [==============================] - 35s 249ms/step - loss: 0.7783 - state_full_binary_fbeta: 0.6224 - state_full_binary_fbeta_macro: 0.6630 - state_full_binary_fbeta_weighted: 0.6630 - val_loss: 0.6189 - val_state_full_binary_fbeta: 0.6690 - val_state_full_binary_fbeta_macro: 0.6792 - val_state_full_binary_fbeta_weighted: 0.6792\n",
            "Epoch 8/15\n",
            "142/142 [==============================] - 35s 249ms/step - loss: 0.7217 - state_full_binary_fbeta: 0.6469 - state_full_binary_fbeta_macro: 0.6630 - state_full_binary_fbeta_weighted: 0.6630 - val_loss: 0.6111 - val_state_full_binary_fbeta: 0.6762 - val_state_full_binary_fbeta_macro: 0.6792 - val_state_full_binary_fbeta_weighted: 0.6792\n",
            "Epoch 9/15\n",
            "142/142 [==============================] - 35s 249ms/step - loss: 0.7093 - state_full_binary_fbeta: 0.6461 - state_full_binary_fbeta_macro: 0.6630 - state_full_binary_fbeta_weighted: 0.6630 - val_loss: 0.6071 - val_state_full_binary_fbeta: 0.6715 - val_state_full_binary_fbeta_macro: 0.6792 - val_state_full_binary_fbeta_weighted: 0.6792\n",
            "Epoch 10/15\n",
            "142/142 [==============================] - 35s 250ms/step - loss: 0.6784 - state_full_binary_fbeta: 0.6454 - state_full_binary_fbeta_macro: 0.6630 - state_full_binary_fbeta_weighted: 0.6630 - val_loss: 0.6016 - val_state_full_binary_fbeta: 0.6655 - val_state_full_binary_fbeta_macro: 0.6792 - val_state_full_binary_fbeta_weighted: 0.6792\n",
            "Epoch 11/15\n",
            "142/142 [==============================] - 35s 249ms/step - loss: 0.6514 - state_full_binary_fbeta: 0.6645 - state_full_binary_fbeta_macro: 0.6630 - state_full_binary_fbeta_weighted: 0.6630 - val_loss: 0.6024 - val_state_full_binary_fbeta: 0.6667 - val_state_full_binary_fbeta_macro: 0.6792 - val_state_full_binary_fbeta_weighted: 0.6792\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history_hindi_lab_cnn_test = model_hindi_two.evaluate(\n",
        "    x=test_ds_hindi,\n",
        "    batch_size=None,\n",
        "    verbose=1,\n",
        "    sample_weight=None,\n",
        "    steps=None,\n",
        "    callbacks=None,\n",
        "    max_queue_size=10,\n",
        "    workers=1,\n",
        "    use_multiprocessing=False,\n",
        "    return_dict=False,\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "append_record({'language':\"hindi_test\", 'history':history_hindi_lab_cnn_test})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sknZC5Mq7FrK",
        "outputId": "e8b6bb99-a4b7-4a00-a058-7daba0a5ae79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18/18 [==============================] - 4s 218ms/step - loss: 0.5694 - state_full_binary_fbeta: 0.7065 - state_full_binary_fbeta_macro: 0.6832 - state_full_binary_fbeta_weighted: 0.6832\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hindi_lab_cnn_preds_test = model_hindi_two.predict(test_ds_hindi, batch_size = 10)\n",
        "hindi_lab_cnn_preds_test_df = pd.DataFrame(hindi_lab_cnn_preds_test, columns=['predicted_test_vals'])\n",
        "\n",
        "hindi_lab_cnn_preds_test_df.to_csv('/content/gdrive/My Drive/266_datasets/standard_hindi_data/hindi_lab_cnn_predictions.csv')"
      ],
      "metadata": {
        "id": "6V0Hj1OYs580"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DO ZERO SHOT LEARNING ON LaBSE WITH CNN MODEL"
      ],
      "metadata": {
        "id": "cMd71XP1tmXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use for F1 metrics on English data\n",
        "statefull_binary_fbeta_english_lab_cnn = StatefullBinaryFBeta() \n",
        "statefull_multi_class_fbeta_english_lab_cnn = StatefullMultiClassFBeta()\n",
        "\n",
        "statefull_multi_class_fbeta_english_lab_cnn_weighted = StatefullMultiClassFBetaWeighted()"
      ],
      "metadata": {
        "id": "E68ZsoEZtlgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_filters = [32, 32, 32, 32, 32]\n",
        "kernel_sizes = [1, 2, 3, 4,5]\n",
        "args = (0.00002, 0,0,0.7)\n",
        "\n",
        "# Return the following metrics\n",
        "kwargs = {\"num_filters\": num_filters, \"kernel_sizes\": kernel_sizes,\"first_metric\": statefull_binary_fbeta_english_lab_cnn, \"second_metric\": statefull_multi_class_fbeta_english_lab_cnn , \"third_metric\": statefull_multi_class_fbeta_english_lab_cnn_weighted}\n",
        "\n",
        "# Return the model and the settings used in model\n",
        "model_english_two, settings_lab_cnn = build_lab_cnn(*args, **kwargs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnV-_kZMuCvt",
        "outputId": "2884d790-3867-4644-9d5f-ac8ee9bbbb6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_english_two.reset_states()\n",
        "history_lab_cnn_english = model_english_two.fit(x=train_ds_english,validation_data=val_ds_english,\n",
        "                               epochs=15, batch_size=30, callbacks=[callback])\n",
        "\n",
        "\n",
        "\n",
        "append_record({'language':\"english\", 'setings':settings_lab_cnn, 'history':history_lab_cnn_english.history})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRw2Rm6xu0KY",
        "outputId": "2f92e03e-25e1-43a9-ce51-0b8a1dcfeeb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "816/816 [==============================] - ETA: 0s - loss: 0.7656 - state_full_binary_fbeta: 0.7032 - state_full_binary_fbeta_macro: 0.6679 - state_full_binary_fbeta_weighted: 0.6679"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2034: UserWarning: Metric StatefullBinaryFBeta implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  m.reset_state()\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2034: UserWarning: Metric StatefullMultiClassFBeta implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  m.reset_state()\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2034: UserWarning: Metric StatefullMultiClassFBetaWeighted implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  m.reset_state()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r816/816 [==============================] - 210s 247ms/step - loss: 0.7656 - state_full_binary_fbeta: 0.7032 - state_full_binary_fbeta_macro: 0.6679 - state_full_binary_fbeta_weighted: 0.6679 - val_loss: 0.3383 - val_state_full_binary_fbeta: 0.8564 - val_state_full_binary_fbeta_macro: 0.6583 - val_state_full_binary_fbeta_weighted: 0.6583\n",
            "Epoch 2/15\n",
            "816/816 [==============================] - 200s 245ms/step - loss: 0.4150 - state_full_binary_fbeta: 0.8224 - state_full_binary_fbeta_macro: 0.6679 - state_full_binary_fbeta_weighted: 0.6679 - val_loss: 0.3018 - val_state_full_binary_fbeta: 0.8691 - val_state_full_binary_fbeta_macro: 0.6583 - val_state_full_binary_fbeta_weighted: 0.6583\n",
            "Epoch 3/15\n",
            "816/816 [==============================] - 200s 245ms/step - loss: 0.3482 - state_full_binary_fbeta: 0.8557 - state_full_binary_fbeta_macro: 0.6679 - state_full_binary_fbeta_weighted: 0.6679 - val_loss: 0.2842 - val_state_full_binary_fbeta: 0.8775 - val_state_full_binary_fbeta_macro: 0.6583 - val_state_full_binary_fbeta_weighted: 0.6583\n",
            "Epoch 4/15\n",
            "816/816 [==============================] - 201s 247ms/step - loss: 0.3141 - state_full_binary_fbeta: 0.8701 - state_full_binary_fbeta_macro: 0.6679 - state_full_binary_fbeta_weighted: 0.6679 - val_loss: 0.2705 - val_state_full_binary_fbeta: 0.8860 - val_state_full_binary_fbeta_macro: 0.6583 - val_state_full_binary_fbeta_weighted: 0.6583\n",
            "Epoch 5/15\n",
            "816/816 [==============================] - 221s 271ms/step - loss: 0.2976 - state_full_binary_fbeta: 0.8762 - state_full_binary_fbeta_macro: 0.6679 - state_full_binary_fbeta_weighted: 0.6679 - val_loss: 0.2635 - val_state_full_binary_fbeta: 0.8891 - val_state_full_binary_fbeta_macro: 0.6583 - val_state_full_binary_fbeta_weighted: 0.6583\n",
            "Epoch 6/15\n",
            "816/816 [==============================] - 217s 266ms/step - loss: 0.2853 - state_full_binary_fbeta: 0.8838 - state_full_binary_fbeta_macro: 0.6679 - state_full_binary_fbeta_weighted: 0.6679 - val_loss: 0.2600 - val_state_full_binary_fbeta: 0.8869 - val_state_full_binary_fbeta_macro: 0.6583 - val_state_full_binary_fbeta_weighted: 0.6583\n",
            "Epoch 7/15\n",
            "816/816 [==============================] - 200s 245ms/step - loss: 0.2743 - state_full_binary_fbeta: 0.8905 - state_full_binary_fbeta_macro: 0.6679 - state_full_binary_fbeta_weighted: 0.6679 - val_loss: 0.2561 - val_state_full_binary_fbeta: 0.8925 - val_state_full_binary_fbeta_macro: 0.6583 - val_state_full_binary_fbeta_weighted: 0.6583\n",
            "Epoch 8/15\n",
            "816/816 [==============================] - 201s 246ms/step - loss: 0.2667 - state_full_binary_fbeta: 0.8919 - state_full_binary_fbeta_macro: 0.6679 - state_full_binary_fbeta_weighted: 0.6679 - val_loss: 0.2518 - val_state_full_binary_fbeta: 0.8941 - val_state_full_binary_fbeta_macro: 0.6583 - val_state_full_binary_fbeta_weighted: 0.6583\n",
            "Epoch 9/15\n",
            "816/816 [==============================] - 200s 245ms/step - loss: 0.2604 - state_full_binary_fbeta: 0.8943 - state_full_binary_fbeta_macro: 0.6679 - state_full_binary_fbeta_weighted: 0.6679 - val_loss: 0.2489 - val_state_full_binary_fbeta: 0.8939 - val_state_full_binary_fbeta_macro: 0.6583 - val_state_full_binary_fbeta_weighted: 0.6583\n",
            "Epoch 10/15\n",
            "816/816 [==============================] - 200s 245ms/step - loss: 0.2554 - state_full_binary_fbeta: 0.8963 - state_full_binary_fbeta_macro: 0.6679 - state_full_binary_fbeta_weighted: 0.6679 - val_loss: 0.2458 - val_state_full_binary_fbeta: 0.8968 - val_state_full_binary_fbeta_macro: 0.6583 - val_state_full_binary_fbeta_weighted: 0.6583\n",
            "Epoch 11/15\n",
            "816/816 [==============================] - 200s 246ms/step - loss: 0.2494 - state_full_binary_fbeta: 0.9012 - state_full_binary_fbeta_macro: 0.6679 - state_full_binary_fbeta_weighted: 0.6679 - val_loss: 0.2454 - val_state_full_binary_fbeta: 0.8957 - val_state_full_binary_fbeta_macro: 0.6583 - val_state_full_binary_fbeta_weighted: 0.6583\n",
            "Epoch 12/15\n",
            "816/816 [==============================] - 200s 245ms/step - loss: 0.2453 - state_full_binary_fbeta: 0.9017 - state_full_binary_fbeta_macro: 0.6679 - state_full_binary_fbeta_weighted: 0.6679 - val_loss: 0.2411 - val_state_full_binary_fbeta: 0.8991 - val_state_full_binary_fbeta_macro: 0.6583 - val_state_full_binary_fbeta_weighted: 0.6583\n",
            "Epoch 13/15\n",
            "816/816 [==============================] - 200s 245ms/step - loss: 0.2378 - state_full_binary_fbeta: 0.9053 - state_full_binary_fbeta_macro: 0.6679 - state_full_binary_fbeta_weighted: 0.6679 - val_loss: 0.2399 - val_state_full_binary_fbeta: 0.8991 - val_state_full_binary_fbeta_macro: 0.6583 - val_state_full_binary_fbeta_weighted: 0.6583\n",
            "Epoch 14/15\n",
            "816/816 [==============================] - 200s 245ms/step - loss: 0.2355 - state_full_binary_fbeta: 0.9064 - state_full_binary_fbeta_macro: 0.6679 - state_full_binary_fbeta_weighted: 0.6679 - val_loss: 0.2382 - val_state_full_binary_fbeta: 0.9006 - val_state_full_binary_fbeta_macro: 0.6583 - val_state_full_binary_fbeta_weighted: 0.6583\n",
            "Epoch 15/15\n",
            "816/816 [==============================] - 200s 245ms/step - loss: 0.2324 - state_full_binary_fbeta: 0.9080 - state_full_binary_fbeta_macro: 0.6679 - state_full_binary_fbeta_weighted: 0.6679 - val_loss: 0.2372 - val_state_full_binary_fbeta: 0.8990 - val_state_full_binary_fbeta_macro: 0.6583 - val_state_full_binary_fbeta_weighted: 0.6583\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Zero- shot. Trained on English and eval on korean test data.\n",
        "history_eng_korean_lab_cnn_test = model_english_two.evaluate(\n",
        "                                                x=test_ds,\n",
        "                                                batch_size=None,\n",
        "                                                verbose=1,\n",
        "                                                sample_weight=None,\n",
        "                                                steps=None,\n",
        "                                                callbacks=None,\n",
        "                                                max_queue_size=10,\n",
        "                                                workers=1,\n",
        "                                                use_multiprocessing=False,\n",
        "                                                return_dict=False,\n",
        "\n",
        "                                                )\n",
        "\n",
        "append_record({'language':\"english_korean_test\", 'history':history_eng_korean_lab_cnn_test})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K20ht_KqyQ8C",
        "outputId": "7182b6fd-6e44-4d86-d392-ac25047e7fef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2034: UserWarning: Metric StatefullBinaryFBeta implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  m.reset_state()\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2034: UserWarning: Metric StatefullMultiClassFBeta implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  m.reset_state()\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2034: UserWarning: Metric StatefullMultiClassFBetaWeighted implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  m.reset_state()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25/25 [==============================] - 5s 209ms/step - loss: 0.7109 - state_full_binary_fbeta: 0.6394 - state_full_binary_fbeta_macro: 0.6618 - state_full_binary_fbeta_weighted: 0.6618\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PREDICT ON KOREAN TEST DATA\n",
        "\n",
        "korean_eng_lab_cnn_preds_test = model_english_two.predict(test_ds, batch_size = 10)\n",
        "korean_eng_lab_cnn_preds_test_df = pd.DataFrame(korean_eng_lab_cnn_preds_test, columns=['predicted_test_vals'])\n",
        "\n",
        "korean_eng_lab_cnn_preds_test_df.to_csv('/content/gdrive/My Drive/266_datasets/standard_english_data/korean_eng_lab_cnn_predictions.csv')"
      ],
      "metadata": {
        "id": "MMv3V_3myr5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zero- shot. Trained on English and eval on hindi test data.\n",
        "history_eng_hindi_lab_cnn_test = model_english_two.evaluate(\n",
        "                                                x=test_ds_hindi,\n",
        "                                                batch_size=None,\n",
        "                                                verbose=1,\n",
        "                                                sample_weight=None,\n",
        "                                                steps=None,\n",
        "                                                callbacks=None,\n",
        "                                                max_queue_size=10,\n",
        "                                                workers=1,\n",
        "                                                use_multiprocessing=False,\n",
        "                                                return_dict=False,\n",
        "\n",
        "                                                )\n",
        "\n",
        "append_record({'language':\"english_hindi_test\", 'history':history_eng_hindi_lab_cnn_test})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8bZ2cATzEf-",
        "outputId": "92e25371-ee60-468a-ea8b-ccb86f70bcf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2034: UserWarning: Metric StatefullBinaryFBeta implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  m.reset_state()\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2034: UserWarning: Metric StatefullMultiClassFBeta implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  m.reset_state()\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2034: UserWarning: Metric StatefullMultiClassFBetaWeighted implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  m.reset_state()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18/18 [==============================] - 4s 214ms/step - loss: 0.7040 - state_full_binary_fbeta: 0.6639 - state_full_binary_fbeta_macro: 0.6832 - state_full_binary_fbeta_weighted: 0.6832\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PREDICT ON HINDI TEST DATA\n",
        "\n",
        "hindi_eng_lab_cnn_preds_test = model_english_two.predict(test_ds_hindi, batch_size = 10)\n",
        "hindi_eng_lab_cnn_preds_test_df = pd.DataFrame(hindi_eng_lab_cnn_preds_test, columns=['predicted_test_vals'])\n",
        "\n",
        "hindi_eng_lab_cnn_preds_test_df.to_csv('/content/gdrive/My Drive/266_datasets/standard_english_data/hindi_eng_lab_cnn_predictions.csv')"
      ],
      "metadata": {
        "id": "TI8l9eWVzRsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HAND TEST F1 SCORE FOR LaBSE with CNN FOR KOREAN"
      ],
      "metadata": {
        "id": "l7q4l7cA3V5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hand check F1 score\n",
        "\n",
        "# Load the Korean Lab CNN test predictions\n",
        "test_df_korean_lab_cnn_predicted = pd.read_csv('/content/gdrive/My Drive/266_datasets/standard_korean_data/korean_lab_cnn_predictions.csv',\n",
        "                       sep = ',', \n",
        "                       dtype={'predicted_test_vals':float}\n",
        "                       )\n",
        "\n",
        "\n",
        "# Turn predictions into 0 and 1\n",
        "test_df_korean_lab_cnn_predicted['Actual Predictions Korean Lab CNN'] = np.where(test_df_korean_lab_cnn_predicted['predicted_test_vals'] < 0.5, 0,  1)\n",
        "\n",
        "# Drop unnamed columns\n",
        "test_df_korean_lab_cnn_predicted = test_df_korean_lab_cnn_predicted.drop(columns=['Unnamed: 0'], axis=1)\n",
        "\n",
        "\n",
        "# Concatenate the correct test data values with predicted data values\n",
        "test_df_korean_lab_cnn_concat = pd.concat([test_df_korean, test_df_korean_lab_cnn_predicted], axis=1)\n",
        "\n",
        "\n",
        "# Find TP\n",
        "test_df_korean_lab_cnn_concat['TP_FP'] = np.where((test_df_korean_lab_cnn_concat['Actual Predictions Korean Lab CNN'] == test_df_korean_lab_cnn_concat['label']) &  (test_df_korean_lab_cnn_concat['label'] ==1), 'TP',  'unknown')\n",
        "\n",
        "\n",
        "# Find FP\n",
        "test_df_korean_lab_cnn_concat['TP_FP'] = np.where((test_df_korean_lab_cnn_concat['Actual Predictions Korean Lab CNN'] == 1) &  (test_df_korean_lab_cnn_concat['label'] ==0), 'FP', test_df_korean_lab_cnn_concat['TP_FP'])\n",
        "\n",
        "\n",
        "# Find FN\n",
        "test_df_korean_lab_cnn_concat['TP_FP'] = np.where((test_df_korean_lab_cnn_concat['Actual Predictions Korean Lab CNN'] == 0) &  (test_df_korean_lab_cnn_concat['label'] ==1), 'FN', test_df_korean_lab_cnn_concat['TP_FP'])\n",
        "\n",
        "# Find TN\n",
        "test_df_korean_lab_cnn_concat['TP_FP'] = np.where((test_df_korean_lab_cnn_concat['Actual Predictions Korean Lab CNN'] == 0) &  (test_df_korean_lab_cnn_concat['label'] ==0), 'TN', test_df_korean_lab_cnn_concat['TP_FP'])\n",
        "\n",
        "print(test_df_korean_lab_cnn_concat.head(20))\n",
        "\n",
        "\n",
        "# Count TP\n",
        "count_tp_korean_lab_cnn = test_df_korean_lab_cnn_concat['TP_FP'].value_counts()['TP']\n",
        "print(\"\\n\\nCount of TP\", count_tp_korean_lab_cnn)\n",
        "\n",
        "\n",
        "# Count FP\n",
        "count_fp_korean_lab_cnn = test_df_korean_lab_cnn_concat['TP_FP'].value_counts()['FP']\n",
        "print(\"Count of FP\", count_fp_korean_lab_cnn)\n",
        "\n",
        "# Count FN\n",
        "count_fn_korean_lab_cnn = test_df_korean_lab_cnn_concat['TP_FP'].value_counts()['FN']\n",
        "print(\"Count of FN\", count_fn_korean_lab_cnn)\n",
        "\n",
        "# Count TN\n",
        "count_tn_korean_lab_cnn = test_df_korean_lab_cnn_concat['TP_FP'].value_counts()['TN']\n",
        "print(\"Count of TN\", count_tn_korean_lab_cnn)\n",
        "\n",
        "# Calculate Precision and Recall and F1 Score\n",
        "precision_korean_lab_cnn = count_tp_korean_lab_cnn / (count_tp_korean_lab_cnn + count_fp_korean_lab_cnn)\n",
        "print(\"precision\", precision_korean_lab_cnn)\n",
        "\n",
        "recall_korean_lab_cnn = count_tp_korean_lab_cnn / (count_tp_korean_lab_cnn  + count_fn_korean_lab_cnn)\n",
        "\n",
        "print(\"recall\", recall_korean_lab_cnn)\n",
        "f1_score_korean_lab_cnn = (2 * precision_korean_lab_cnn * recall_korean_lab_cnn) / (precision_korean_lab_cnn  + recall_korean_lab_cnn)\n",
        "\n",
        "\n",
        "print(\"F1 score is: \", f1_score_korean_lab_cnn )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXdwf6pKXE3K",
        "outputId": "94332d22-0883-4b84-ba4a-bb32d91c7772"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             comments  label  \\\n",
            "0   팀으로 데뷔한거면 개인활동 했어도 N빵 해야지... 그게 팀을 위해서도 맞는거고~~...      0   \n",
            "1                   왕지혜 34살이지만 외모는 인정한다.여자라면 이정도는 되야지      0   \n",
            "2                            여자들이 80프로잉 악플 남자는 여자욕 안해      1   \n",
            "3                      설현이 떨고 있다... 아니겠지 아닐거야 그것만은 안돼      0   \n",
            "4                  다된 기생충 잔치에재 뿌린 방가방송과 안현모 다신 나서지 말자      1   \n",
            "5                              광규형님 성국이형팬입니다 꼭좀 전해주세요      0   \n",
            "6                    남자가 사회 생활하다 보면 손이 스칠수도 있는거지 거참~~      1   \n",
            "7         여자도 출신이 의심스러움.사랑했던관계라기보다는남자발목잡으려고 그러는거같은데..      1   \n",
            "8                                            ? 대세배우 ?      0   \n",
            "9                                 개그맨 김경민 닮은다고 나만 느낌?      0   \n",
            "10                                   중환자실이면 살아도 죽은것이여      0   \n",
            "11  현실적으로 둘다 유명하지 않아서 수입도 불안정하고.. 그냥 헤어지자 해서 헤어지는 듯..      0   \n",
            "12  이나영 보며 이쁘다..이쁘다..역시이쁘다..어쩜 저리 하나도 안변하고 아이낳고두 이...      0   \n",
            "13  ㅋㅋ 역시 1화 방영후 말들이 많네 참고로 미스터션샤인도 1화 나오고 재미없다느니 ...      0   \n",
            "14                                     이 드라마엔 여주가 없군.      0   \n",
            "15  그만좀해라사람죽겄다기자들도기사 그만내라사람하나 죽어나가야 멈출건가벌받고 나올거다알권...      1   \n",
            "16                            중기 병헌 현빈은 같은 작품 하기 껄그럽겠      1   \n",
            "17                    부럽다. 나도 소간지랑 저 집에서 살고.....시프네요ㅠ      0   \n",
            "18                                     사진은 수지닮았네 이뻐요~      0   \n",
            "19                근데 솔직히 김정훈 천재유전자 받고 싶어하는 여자들 많을텐데..      1   \n",
            "\n",
            "    predicted_test_vals  Actual Predictions Korean Lab CNN TP_FP  \n",
            "0              0.344278                                  0    TN  \n",
            "1              0.602851                                  1    FP  \n",
            "2              0.939980                                  1    TP  \n",
            "3              0.414587                                  0    TN  \n",
            "4              0.777714                                  1    TP  \n",
            "5              0.383980                                  0    TN  \n",
            "6              0.614098                                  1    TP  \n",
            "7              0.757910                                  1    TP  \n",
            "8              0.556036                                  1    FP  \n",
            "9              0.241707                                  0    TN  \n",
            "10             0.575069                                  1    FP  \n",
            "11             0.571528                                  1    FP  \n",
            "12             0.152026                                  0    TN  \n",
            "13             0.478110                                  0    TN  \n",
            "14             0.408027                                  0    TN  \n",
            "15             0.831104                                  1    TP  \n",
            "16             0.401276                                  0    FN  \n",
            "17             0.356537                                  0    TN  \n",
            "18             0.119124                                  0    TN  \n",
            "19             0.465388                                  0    FN  \n",
            "\n",
            "\n",
            "Count of TP 252\n",
            "Count of FP 99\n",
            "Count of FN 109\n",
            "Count of TN 270\n",
            "precision 0.717948717948718\n",
            "recall 0.6980609418282548\n",
            "F1 score is:  0.7078651685393258\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HAND TEST F1 SCORE FOR LaBSE PLUS CNN FOR HINDI"
      ],
      "metadata": {
        "id": "0_7WAQBe3ga_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hand check F1 score for Hindi Lab CNN Model\n",
        "\n",
        "\n",
        "# Load the Korean Lab CNN test predictions\n",
        "test_df_hindi_lab_cnn_predicted = pd.read_csv('/content/gdrive/My Drive/266_datasets/standard_hindi_data/hindi_lab_cnn_predictions.csv',\n",
        "                       sep = ',', \n",
        "                       dtype={'predicted_test_vals':float}\n",
        "                       )\n",
        "\n",
        "\n",
        "# Turn predictions into 0 and 1\n",
        "test_df_hindi_lab_cnn_predicted['Actual Predictions Hindi Lab CNN'] = np.where(test_df_hindi_lab_cnn_predicted['predicted_test_vals'] < 0.5, 0,  1)\n",
        "\n",
        "# Drop unnamed columns\n",
        "test_df_hindi_lab_cnn_predicted = test_df_hindi_lab_cnn_predicted.drop(columns=['Unnamed: 0'], axis=1)\n",
        "\n",
        "\n",
        "# Concatenate the correct test data values with predicted data values\n",
        "test_df_hindi_lab_cnn_concat = pd.concat([test_df_hindi, test_df_hindi_lab_cnn_predicted], axis=1)\n",
        "\n",
        "\n",
        "# Find TP\n",
        "test_df_hindi_lab_cnn_concat['TP_FP'] = np.where((test_df_hindi_lab_cnn_concat['Actual Predictions Hindi Lab CNN'] == test_df_hindi_lab_cnn_concat['label']) &  (test_df_hindi_lab_cnn_concat['label'] ==1), 'TP',  'unknown')\n",
        "\n",
        "\n",
        "# Find FP\n",
        "test_df_hindi_lab_cnn_concat['TP_FP'] = np.where((test_df_hindi_lab_cnn_concat['Actual Predictions Hindi Lab CNN'] == 1) &  (test_df_hindi_lab_cnn_concat['label'] ==0), 'FP', test_df_hindi_lab_cnn_concat['TP_FP'])\n",
        "\n",
        "\n",
        "# Find FN\n",
        "test_df_hindi_lab_cnn_concat['TP_FP'] = np.where((test_df_hindi_lab_cnn_concat['Actual Predictions Hindi Lab CNN'] == 0) &  (test_df_hindi_lab_cnn_concat['label'] ==1), 'FN', test_df_hindi_lab_cnn_concat['TP_FP'])\n",
        "\n",
        "# Find TN\n",
        "test_df_hindi_lab_cnn_concat['TP_FP'] = np.where((test_df_hindi_lab_cnn_concat['Actual Predictions Hindi Lab CNN'] == 0) &  (test_df_hindi_lab_cnn_concat['label'] ==0), 'TN', test_df_hindi_lab_cnn_concat['TP_FP'])\n",
        "print(test_df_hindi_lab_cnn_concat.head(20))\n",
        "\n",
        "\n",
        "# Count TP\n",
        "count_tp_hindi_lab_cnn = test_df_hindi_lab_cnn_concat['TP_FP'].value_counts()['TP']\n",
        "print(\"\\n\\nCount of TP\",count_tp_hindi_lab_cnn)\n",
        "\n",
        "\n",
        "# Count FP\n",
        "count_fp_hindi_lab_cnn = test_df_hindi_lab_cnn_concat['TP_FP'].value_counts()['FP']\n",
        "print(\"Count of FP\", count_fp_hindi_lab_cnn)\n",
        "\n",
        "# Count FN\n",
        "count_fn_hindi_lab_cnn = test_df_hindi_lab_cnn_concat['TP_FP'].value_counts()['FN']\n",
        "print(\"Count of FN\",count_fn_hindi_lab_cnn)\n",
        "\n",
        "\n",
        "# Count TN\n",
        "count_tn_hindi_lab_cnn = test_df_hindi_lab_cnn_concat['TP_FP'].value_counts()['TN']\n",
        "print(\"Count of TN\",count_tn_hindi_lab_cnn)\n",
        "\n",
        "\n",
        "# Calculate Precision and Recall and F1 Score\n",
        "precision_hindi_lab_cnn = count_tp_hindi_lab_cnn / (count_tp_hindi_lab_cnn + count_fp_hindi_lab_cnn)\n",
        "print(\"precision\", precision_hindi_lab_cnn)\n",
        "\n",
        "recall_hindi_lab_cnn = count_tp_hindi_lab_cnn / (count_tp_hindi_lab_cnn  + count_fn_hindi_lab_cnn)\n",
        "\n",
        "print(\"recall\", recall_hindi_lab_cnn)\n",
        "f1_score_hindi_lab_cnn = (2 * precision_hindi_lab_cnn * recall_hindi_lab_cnn) / (precision_hindi_lab_cnn  + recall_hindi_lab_cnn)\n",
        "\n",
        "\n",
        "print(\"\\n\\nF1 score for Hindi Lab CNN is: \", f1_score_hindi_lab_cnn )\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Tp-hL_XomwH",
        "outputId": "9c14fd5b-55d3-4188-a596-44e762f67f8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             comments  label  \\\n",
            "0   @zishanAliRJD @iAnantSingh_ *ओसामा साहब ने सिर...      0   \n",
            "1   @China_Amb_India @narendramodi I am shocked th...      0   \n",
            "2   कल से 18 से ऊपर वालो को हवा की वैक्सीन लगेगी.....      0   \n",
            "3   इधर की बात उधर करने में \\nआज भी जीमेल  से आगे ...      0   \n",
            "4   ☯️ मोदी जी ➡️ सोंगंद मुझे इस मिट्टी की मै देश ...      1   \n",
            "5   कबीर परमेश्वर जी ने अध्यात्म का विधान बताया है...      0   \n",
            "6   1990 रिपीट हो चुका है,1 लाख से ज्यादा हिन्दू ल...      0   \n",
            "7   \"MODI MEANS TERROR\"  #नरेंद्र_मोदी_ग्लोबल_पप्प...      1   \n",
            "8                      @Asrathour_143 और हिंदी में 😜😜      0   \n",
            "9   केंद्र में होते हुए भी, यदि आप कुछ नहीं कर पा ...      0   \n",
            "10  पप्पू लाल  इस लिए हाथ दिखा रहा है वह जानता है ...      1   \n",
            "11  @chitraaum बहन आपसे उम्मित हे आप इस विषय पर सं...      0   \n",
            "12  RT @MdNadim_BGS: ₹21000Cr की लागत से बनी भारत ...      0   \n",
            "13  It’s Time To Come Together To Save Farmers &am...      0   \n",
            "14  @anandkalra69 @Vandana__Indian हिन्दू इतने हरा...      1   \n",
            "15  @Naseeruddin_sah लालू की रिहाई की कीमत शहाबुद्...      0   \n",
            "16    इस की सकल देख कर इस को कोई रखेल भी नही बनायेगा।      1   \n",
            "17  इनको मांही खाना है, तो ये ससुरे सुअर, सुअर का ...      1   \n",
            "18  Adv Badshah Sadakat भाई का RJD से इस्तीफा!!! ह...      0   \n",
            "19  @TheOfficialSBI भाड़ मे जाओ एसबीआई वालों मैं त...      0   \n",
            "\n",
            "    predicted_test_vals  Actual Predictions Hindi Lab CNN TP_FP  \n",
            "0              0.878002                                 1    FP  \n",
            "1              0.518621                                 1    FP  \n",
            "2              0.113499                                 0    TN  \n",
            "3              0.424176                                 0    TN  \n",
            "4              0.658142                                 1    TP  \n",
            "5              0.515586                                 1    FP  \n",
            "6              0.615638                                 1    FP  \n",
            "7              0.271316                                 0    FN  \n",
            "8              0.426196                                 0    TN  \n",
            "9              0.095965                                 0    TN  \n",
            "10             0.510524                                 1    TP  \n",
            "11             0.450351                                 0    TN  \n",
            "12             0.422852                                 0    TN  \n",
            "13             0.176586                                 0    TN  \n",
            "14             0.825162                                 1    TP  \n",
            "15             0.179218                                 0    TN  \n",
            "16             0.529221                                 1    TP  \n",
            "17             0.229874                                 0    FN  \n",
            "18             0.194324                                 0    TN  \n",
            "19             0.409112                                 0    TN  \n",
            "\n",
            "\n",
            "Count of TP 189\n",
            "Count of FP 70\n",
            "Count of FN 86\n",
            "Count of TN 185\n",
            "precision 0.7297297297297297\n",
            "recall 0.6872727272727273\n",
            "\n",
            "\n",
            "F1 score for Hindi Lab CNN is:  0.7078651685393259\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HAND TEST F1 SCORE LABSE OPTIMIZED HYPERPARAMETERS - KOREAN"
      ],
      "metadata": {
        "id": "CsSpSJLM4Hm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hand check F1 score\n",
        "\n",
        "# Load the Korean Lab  test predictions\n",
        "test_df_korean_lab_predicted = pd.read_csv('/content/gdrive/My Drive/266_datasets/standard_korean_data/korean_lab_predictions_two.csv',\n",
        "                       sep = ',', \n",
        "                       dtype={'predicted_test_vals':float}\n",
        "                       )\n",
        "\n",
        "\n",
        "# Turn predictions into 0 and 1\n",
        "test_df_korean_lab_predicted['Actual Predictions Korean Lab'] = np.where(test_df_korean_lab_predicted['predicted_test_vals'] < 0.5, 0,  1)\n",
        "\n",
        "# Drop unnamed columns\n",
        "test_df_korean_lab_predicted = test_df_korean_lab_predicted.drop(columns=['Unnamed: 0'], axis=1)\n",
        "\n",
        "\n",
        "# Concatenate the correct test data values with predicted data values\n",
        "test_df_korean_lab_concat = pd.concat([test_df_korean, test_df_korean_lab_predicted], axis=1)\n",
        "\n",
        "\n",
        "# Find TP\n",
        "test_df_korean_lab_concat['TP_FP'] = np.where((test_df_korean_lab_concat['Actual Predictions Korean Lab'] == test_df_korean_lab_concat['label']) &  (test_df_korean_lab_concat['label'] ==1), 'TP',  'unknown')\n",
        "\n",
        "\n",
        "# Find FP\n",
        "test_df_korean_lab_concat['TP_FP'] = np.where((test_df_korean_lab_concat['Actual Predictions Korean Lab'] == 1) &  (test_df_korean_lab_concat['label'] ==0), 'FP', test_df_korean_lab_concat['TP_FP'])\n",
        "\n",
        "\n",
        "# Find FN\n",
        "test_df_korean_lab_concat['TP_FP'] = np.where((test_df_korean_lab_concat['Actual Predictions Korean Lab'] == 0) &  (test_df_korean_lab_concat['label'] ==1), 'FN', test_df_korean_lab_concat['TP_FP'])\n",
        "\n",
        "\n",
        "# Find TN\n",
        "test_df_korean_lab_concat['TP_FP'] = np.where((test_df_korean_lab_concat['Actual Predictions Korean Lab'] == 0) &  (test_df_korean_lab_concat['label'] ==0), 'TN', test_df_korean_lab_concat['TP_FP'])\n",
        "\n",
        "print(test_df_korean_lab_concat.head(20))\n",
        "\n",
        "\n",
        "# Count TP\n",
        "count_tp_korean_lab = test_df_korean_lab_concat['TP_FP'].value_counts()['TP']\n",
        "print(\"\\n\\nCount of TP\", count_tp_korean_lab)\n",
        "\n",
        "\n",
        "# Count FP\n",
        "count_fp_korean_lab = test_df_korean_lab_concat['TP_FP'].value_counts()['FP']\n",
        "print(\"Count of FP\", count_fp_korean_lab)\n",
        "\n",
        "# Count FN\n",
        "count_fn_korean_lab = test_df_korean_lab_concat['TP_FP'].value_counts()['FN']\n",
        "print(\"Count of FN\", count_fn_korean_lab)\n",
        "\n",
        "# Count TN\n",
        "count_tn_korean_lab = test_df_korean_lab_concat['TP_FP'].value_counts()['TN']\n",
        "print(\"Count of TN\", count_tn_korean_lab)\n",
        "\n",
        "# Calculate Precision and Recall and F1 Score\n",
        "precision_korean_lab = count_tp_korean_lab / (count_tp_korean_lab + count_fp_korean_lab)\n",
        "print(\"precision\", precision_korean_lab)\n",
        "\n",
        "recall_korean_lab = count_tp_korean_lab / (count_tp_korean_lab  + count_fn_korean_lab)\n",
        "\n",
        "print(\"recall\", recall_korean_lab)\n",
        "f1_score_korean_lab = (2 * precision_korean_lab * recall_korean_lab) / (precision_korean_lab  + recall_korean_lab)\n",
        "\n",
        "\n",
        "print(\"F1 score is: \", f1_score_korean_lab )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcE29HsZ4HLw",
        "outputId": "325d050e-6ae1-4904-df52-94e3f29ddb93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             comments  label  \\\n",
            "0   팀으로 데뷔한거면 개인활동 했어도 N빵 해야지... 그게 팀을 위해서도 맞는거고~~...      0   \n",
            "1                   왕지혜 34살이지만 외모는 인정한다.여자라면 이정도는 되야지      0   \n",
            "2                            여자들이 80프로잉 악플 남자는 여자욕 안해      1   \n",
            "3                      설현이 떨고 있다... 아니겠지 아닐거야 그것만은 안돼      0   \n",
            "4                  다된 기생충 잔치에재 뿌린 방가방송과 안현모 다신 나서지 말자      1   \n",
            "5                              광규형님 성국이형팬입니다 꼭좀 전해주세요      0   \n",
            "6                    남자가 사회 생활하다 보면 손이 스칠수도 있는거지 거참~~      1   \n",
            "7         여자도 출신이 의심스러움.사랑했던관계라기보다는남자발목잡으려고 그러는거같은데..      1   \n",
            "8                                            ? 대세배우 ?      0   \n",
            "9                                 개그맨 김경민 닮은다고 나만 느낌?      0   \n",
            "10                                   중환자실이면 살아도 죽은것이여      0   \n",
            "11  현실적으로 둘다 유명하지 않아서 수입도 불안정하고.. 그냥 헤어지자 해서 헤어지는 듯..      0   \n",
            "12  이나영 보며 이쁘다..이쁘다..역시이쁘다..어쩜 저리 하나도 안변하고 아이낳고두 이...      0   \n",
            "13  ㅋㅋ 역시 1화 방영후 말들이 많네 참고로 미스터션샤인도 1화 나오고 재미없다느니 ...      0   \n",
            "14                                     이 드라마엔 여주가 없군.      0   \n",
            "15  그만좀해라사람죽겄다기자들도기사 그만내라사람하나 죽어나가야 멈출건가벌받고 나올거다알권...      1   \n",
            "16                            중기 병헌 현빈은 같은 작품 하기 껄그럽겠      1   \n",
            "17                    부럽다. 나도 소간지랑 저 집에서 살고.....시프네요ㅠ      0   \n",
            "18                                     사진은 수지닮았네 이뻐요~      0   \n",
            "19                근데 솔직히 김정훈 천재유전자 받고 싶어하는 여자들 많을텐데..      1   \n",
            "\n",
            "    predicted_test_vals  Actual Predictions Korean Lab TP_FP  \n",
            "0              0.480018                              0    TN  \n",
            "1              0.823669                              1    FP  \n",
            "2              0.985263                              1    TP  \n",
            "3              0.611882                              1    FP  \n",
            "4              0.980416                              1    TP  \n",
            "5              0.434554                              0    TN  \n",
            "6              0.493339                              0    FN  \n",
            "7              0.971608                              1    TP  \n",
            "8              0.785944                              1    FP  \n",
            "9              0.060874                              0    TN  \n",
            "10             0.049600                              0    TN  \n",
            "11             0.349141                              0    TN  \n",
            "12             0.026095                              0    TN  \n",
            "13             0.093922                              0    TN  \n",
            "14             0.323456                              0    TN  \n",
            "15             0.947983                              1    TP  \n",
            "16             0.671587                              1    TP  \n",
            "17             0.718951                              1    FP  \n",
            "18             0.025089                              0    TN  \n",
            "19             0.346272                              0    FN  \n",
            "\n",
            "\n",
            "Count of TP 260\n",
            "Count of FP 128\n",
            "Count of FN 101\n",
            "Count of TN 241\n",
            "precision 0.6701030927835051\n",
            "recall 0.7202216066481995\n",
            "F1 score is:  0.6942590120160212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HAND CALCULATE LaBSE ONLY OPTIMIZED HYPERPARAMETERS - HINDI"
      ],
      "metadata": {
        "id": "jbIOPTLD71h6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hand check F1 score for Hindi Lab Model\n",
        "\n",
        "\n",
        "# Load the Korean Lab CNN test predictions\n",
        "test_df_hindi_lab_predicted = pd.read_csv('/content/gdrive/My Drive/266_datasets/standard_hindi_data/hindi_lab_predictions_two.csv',\n",
        "                       sep = ',', \n",
        "                       dtype={'predicted_test_vals':float}\n",
        "                       )\n",
        "\n",
        "\n",
        "# Turn predictions into 0 and 1\n",
        "test_df_hindi_lab_predicted['Actual Predictions Hindi Lab'] = np.where(test_df_hindi_lab_predicted['predicted_test_vals'] < 0.5, 0,  1)\n",
        "\n",
        "# Drop unnamed columns\n",
        "test_df_hindi_lab_predicted = test_df_hindi_lab_predicted.drop(columns=['Unnamed: 0'], axis=1)\n",
        "\n",
        "\n",
        "# Concatenate the correct test data values with predicted data values\n",
        "test_df_hindi_lab_concat = pd.concat([test_df_hindi, test_df_hindi_lab_predicted], axis=1)\n",
        "\n",
        "\n",
        "# Find TP\n",
        "test_df_hindi_lab_concat['TP_FP'] = np.where((test_df_hindi_lab_concat['Actual Predictions Hindi Lab'] == test_df_hindi_lab_concat['label']) &  (test_df_hindi_lab_concat['label'] ==1), 'TP',  'unknown')\n",
        "\n",
        "\n",
        "# Find FP\n",
        "test_df_hindi_lab_concat['TP_FP'] = np.where((test_df_hindi_lab_concat['Actual Predictions Hindi Lab'] == 1) &  (test_df_hindi_lab_concat['label'] ==0), 'FP', test_df_hindi_lab_concat['TP_FP'])\n",
        "\n",
        "\n",
        "# Find FN\n",
        "test_df_hindi_lab_concat['TP_FP'] = np.where((test_df_hindi_lab_concat['Actual Predictions Hindi Lab'] == 0) &  (test_df_hindi_lab_concat['label'] ==1), 'FN', test_df_hindi_lab_concat['TP_FP'])\n",
        "\n",
        "\n",
        "# Find TN\n",
        "test_df_hindi_lab_concat['TP_FP'] = np.where((test_df_hindi_lab_concat['Actual Predictions Hindi Lab'] == 0) &  (test_df_hindi_lab_concat['label'] ==0), 'TN', test_df_hindi_lab_concat['TP_FP'])\n",
        "\n",
        "print(test_df_hindi_lab_concat.head(20))\n",
        "\n",
        "\n",
        "# Count TP\n",
        "count_tp_hindi_lab = test_df_hindi_lab_concat['TP_FP'].value_counts()['TP']\n",
        "print(\"\\n\\nCount of TP\",count_tp_hindi_lab)\n",
        "\n",
        "\n",
        "# Count FP\n",
        "count_fp_hindi_lab = test_df_hindi_lab_concat['TP_FP'].value_counts()['FP']\n",
        "print(\"Count of FP\", count_fp_hindi_lab)\n",
        "\n",
        "# Count FN\n",
        "count_fn_hindi_lab = test_df_hindi_lab_concat['TP_FP'].value_counts()['FN']\n",
        "print(\"Count of FN\",count_fn_hindi_lab)\n",
        "\n",
        "# Count TN\n",
        "count_tn_hindi_lab = test_df_hindi_lab_concat['TP_FP'].value_counts()['TN']\n",
        "print(\"Count of TN\",count_tn_hindi_lab)\n",
        "\n",
        "# Calculate Precision and Recall and F1 Score\n",
        "precision_hindi_lab = count_tp_hindi_lab / (count_tp_hindi_lab + count_fp_hindi_lab)\n",
        "print(\"precision\", precision_hindi_lab)\n",
        "\n",
        "recall_hindi_lab = count_tp_hindi_lab / (count_tp_hindi_lab  + count_fn_hindi_lab)\n",
        "\n",
        "print(\"recall\", recall_hindi_lab)\n",
        "f1_score_hindi_lab = (2 * precision_hindi_lab * recall_hindi_lab) / (precision_hindi_lab  + recall_hindi_lab)\n",
        "\n",
        "\n",
        "print(\"\\n\\nF1 score for Hindi Lab is: \", f1_score_hindi_lab )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19o7J0dF7zgG",
        "outputId": "050a0e1f-34b9-4ed0-f73a-80c01f236878"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             comments  label  \\\n",
            "0   @zishanAliRJD @iAnantSingh_ *ओसामा साहब ने सिर...      0   \n",
            "1   @China_Amb_India @narendramodi I am shocked th...      0   \n",
            "2   कल से 18 से ऊपर वालो को हवा की वैक्सीन लगेगी.....      0   \n",
            "3   इधर की बात उधर करने में \\nआज भी जीमेल  से आगे ...      0   \n",
            "4   ☯️ मोदी जी ➡️ सोंगंद मुझे इस मिट्टी की मै देश ...      1   \n",
            "5   कबीर परमेश्वर जी ने अध्यात्म का विधान बताया है...      0   \n",
            "6   1990 रिपीट हो चुका है,1 लाख से ज्यादा हिन्दू ल...      0   \n",
            "7   \"MODI MEANS TERROR\"  #नरेंद्र_मोदी_ग्लोबल_पप्प...      1   \n",
            "8                      @Asrathour_143 और हिंदी में 😜😜      0   \n",
            "9   केंद्र में होते हुए भी, यदि आप कुछ नहीं कर पा ...      0   \n",
            "10  पप्पू लाल  इस लिए हाथ दिखा रहा है वह जानता है ...      1   \n",
            "11  @chitraaum बहन आपसे उम्मित हे आप इस विषय पर सं...      0   \n",
            "12  RT @MdNadim_BGS: ₹21000Cr की लागत से बनी भारत ...      0   \n",
            "13  It’s Time To Come Together To Save Farmers &am...      0   \n",
            "14  @anandkalra69 @Vandana__Indian हिन्दू इतने हरा...      1   \n",
            "15  @Naseeruddin_sah लालू की रिहाई की कीमत शहाबुद्...      0   \n",
            "16    इस की सकल देख कर इस को कोई रखेल भी नही बनायेगा।      1   \n",
            "17  इनको मांही खाना है, तो ये ससुरे सुअर, सुअर का ...      1   \n",
            "18  Adv Badshah Sadakat भाई का RJD से इस्तीफा!!! ह...      0   \n",
            "19  @TheOfficialSBI भाड़ मे जाओ एसबीआई वालों मैं त...      0   \n",
            "\n",
            "    predicted_test_vals  Actual Predictions Hindi Lab TP_FP  \n",
            "0              0.473302                             0    TN  \n",
            "1              0.698759                             1    FP  \n",
            "2              0.054247                             0    TN  \n",
            "3              0.762390                             1    FP  \n",
            "4              0.463459                             0    FN  \n",
            "5              0.642278                             1    FP  \n",
            "6              0.805962                             1    FP  \n",
            "7              0.495538                             0    FN  \n",
            "8              0.123997                             0    TN  \n",
            "9              0.003508                             0    TN  \n",
            "10             0.598582                             1    TP  \n",
            "11             0.647224                             1    FP  \n",
            "12             0.168567                             0    TN  \n",
            "13             0.403265                             0    TN  \n",
            "14             0.789880                             1    TP  \n",
            "15             0.162971                             0    TN  \n",
            "16             0.018896                             0    FN  \n",
            "17             0.352769                             0    FN  \n",
            "18             0.641554                             1    FP  \n",
            "19             0.147502                             0    TN  \n",
            "\n",
            "\n",
            "Count of TP 192\n",
            "Count of FP 65\n",
            "Count of FN 83\n",
            "Count of TN 190\n",
            "precision 0.7470817120622568\n",
            "recall 0.6981818181818182\n",
            "\n",
            "\n",
            "F1 score for Hindi Lab is:  0.7218045112781954\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HAND TEST F1 SCORE FOR KOREAN LABSE BASELINE PARAMETER MODEL"
      ],
      "metadata": {
        "id": "zZvOHmi-cFsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hand check F1 score\n",
        "\n",
        "# Load the Korean Lab  test predictions\n",
        "test_df_korean_lab_base_predicted = pd.read_csv('/content/gdrive/My Drive/266_datasets/standard_korean_data/korean_lab_predictions.csv',\n",
        "                       sep = ',', \n",
        "                       dtype={'predicted_test_vals':float}\n",
        "                       )\n",
        "\n",
        "\n",
        "# Turn predictions into 0 and 1\n",
        "test_df_korean_lab_base_predicted['Actual Predictions Korean Lab Base'] = np.where(test_df_korean_lab_base_predicted['predicted_test_vals'] < 0.5, 0,  1)\n",
        "\n",
        "# Drop unnamed columns\n",
        "test_df_korean_lab_base_predicted = test_df_korean_lab_base_predicted.drop(columns=['Unnamed: 0'], axis=1)\n",
        "\n",
        "\n",
        "# Concatenate the correct test data values with predicted data values\n",
        "test_df_korean_lab_base_concat = pd.concat([test_df_korean, test_df_korean_lab_base_predicted], axis=1)\n",
        "\n",
        "\n",
        "# Find TP\n",
        "test_df_korean_lab_base_concat['TP_FP'] = np.where((test_df_korean_lab_base_concat['Actual Predictions Korean Lab Base'] == test_df_korean_lab_base_concat['label']) &  (test_df_korean_lab_base_concat['label'] ==1), 'TP',  'unknown')\n",
        "\n",
        "\n",
        "# Find FP\n",
        "test_df_korean_lab_base_concat['TP_FP'] = np.where((test_df_korean_lab_base_concat['Actual Predictions Korean Lab Base'] == 1) &  (test_df_korean_lab_base_concat['label'] ==0), 'FP', test_df_korean_lab_base_concat['TP_FP'])\n",
        "\n",
        "\n",
        "# Find FN\n",
        "test_df_korean_lab_base_concat['TP_FP'] = np.where((test_df_korean_lab_base_concat['Actual Predictions Korean Lab Base'] == 0) &  (test_df_korean_lab_base_concat['label'] ==1), 'FN', test_df_korean_lab_base_concat['TP_FP'])\n",
        "\n",
        "\n",
        "# Find TN\n",
        "test_df_korean_lab_base_concat['TP_FP'] = np.where((test_df_korean_lab_base_concat['Actual Predictions Korean Lab Base'] == 0) &  (test_df_korean_lab_base_concat['label'] ==0), 'TN', test_df_korean_lab_base_concat['TP_FP'])\n",
        "\n",
        "print(test_df_korean_lab_base_concat.head(20))\n",
        "\n",
        "\n",
        "# Count TP\n",
        "count_tp_korean_lab_base = test_df_korean_lab_base_concat['TP_FP'].value_counts()['TP']\n",
        "print(\"\\n\\nCount of TP\", count_tp_korean_lab_base)\n",
        "\n",
        "\n",
        "# Count FP\n",
        "count_fp_korean_lab_base = test_df_korean_lab_base_concat['TP_FP'].value_counts()['FP']\n",
        "print(\"Count of FP\", count_fp_korean_lab_base)\n",
        "\n",
        "# Count FN\n",
        "count_fn_korean_lab_base = test_df_korean_lab_base_concat['TP_FP'].value_counts()['FN']\n",
        "print(\"Count of FN\", count_fn_korean_lab_base)\n",
        "\n",
        "# Count TN\n",
        "count_tn_korean_lab_base = test_df_korean_lab_base_concat['TP_FP'].value_counts()['TN']\n",
        "print(\"Count of TN\", count_tn_korean_lab_base)\n",
        "\n",
        "# Calculate Precision and Recall and F1 Score\n",
        "precision_korean_lab_base = count_tp_korean_lab_base / (count_tp_korean_lab_base + count_fp_korean_lab_base)\n",
        "print(\"precision\", precision_korean_lab_base)\n",
        "\n",
        "recall_korean_lab_base = count_tp_korean_lab_base / (count_tp_korean_lab_base  + count_fn_korean_lab_base)\n",
        "\n",
        "print(\"recall\", recall_korean_lab_base)\n",
        "f1_score_korean_lab_base = (2 * precision_korean_lab_base * recall_korean_lab_base) / (precision_korean_lab_base  + recall_korean_lab_base)\n",
        "\n",
        "\n",
        "print(\"F1 score is: \", f1_score_korean_lab_base )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0UutXhdcDol",
        "outputId": "16be7af0-5280-4069-8eac-8ab511b2685d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             comments  label  \\\n",
            "0   팀으로 데뷔한거면 개인활동 했어도 N빵 해야지... 그게 팀을 위해서도 맞는거고~~...      0   \n",
            "1                   왕지혜 34살이지만 외모는 인정한다.여자라면 이정도는 되야지      0   \n",
            "2                            여자들이 80프로잉 악플 남자는 여자욕 안해      1   \n",
            "3                      설현이 떨고 있다... 아니겠지 아닐거야 그것만은 안돼      0   \n",
            "4                  다된 기생충 잔치에재 뿌린 방가방송과 안현모 다신 나서지 말자      1   \n",
            "5                              광규형님 성국이형팬입니다 꼭좀 전해주세요      0   \n",
            "6                    남자가 사회 생활하다 보면 손이 스칠수도 있는거지 거참~~      1   \n",
            "7         여자도 출신이 의심스러움.사랑했던관계라기보다는남자발목잡으려고 그러는거같은데..      1   \n",
            "8                                            ? 대세배우 ?      0   \n",
            "9                                 개그맨 김경민 닮은다고 나만 느낌?      0   \n",
            "10                                   중환자실이면 살아도 죽은것이여      0   \n",
            "11  현실적으로 둘다 유명하지 않아서 수입도 불안정하고.. 그냥 헤어지자 해서 헤어지는 듯..      0   \n",
            "12  이나영 보며 이쁘다..이쁘다..역시이쁘다..어쩜 저리 하나도 안변하고 아이낳고두 이...      0   \n",
            "13  ㅋㅋ 역시 1화 방영후 말들이 많네 참고로 미스터션샤인도 1화 나오고 재미없다느니 ...      0   \n",
            "14                                     이 드라마엔 여주가 없군.      0   \n",
            "15  그만좀해라사람죽겄다기자들도기사 그만내라사람하나 죽어나가야 멈출건가벌받고 나올거다알권...      1   \n",
            "16                            중기 병헌 현빈은 같은 작품 하기 껄그럽겠      1   \n",
            "17                    부럽다. 나도 소간지랑 저 집에서 살고.....시프네요ㅠ      0   \n",
            "18                                     사진은 수지닮았네 이뻐요~      0   \n",
            "19                근데 솔직히 김정훈 천재유전자 받고 싶어하는 여자들 많을텐데..      1   \n",
            "\n",
            "    predicted_test_vals  Actual Predictions Korean Lab Base TP_FP  \n",
            "0              0.073717                                   0    TN  \n",
            "1              0.971796                                   1    FP  \n",
            "2              0.997700                                   1    TP  \n",
            "3              0.017122                                   0    TN  \n",
            "4              0.726807                                   1    TP  \n",
            "5              0.264889                                   0    TN  \n",
            "6              0.937140                                   1    TP  \n",
            "7              0.203925                                   0    FN  \n",
            "8              0.952116                                   1    FP  \n",
            "9              0.016046                                   0    TN  \n",
            "10             0.229296                                   0    TN  \n",
            "11             0.012150                                   0    TN  \n",
            "12             0.023885                                   0    TN  \n",
            "13             0.357643                                   0    TN  \n",
            "14             0.011962                                   0    TN  \n",
            "15             0.250432                                   0    FN  \n",
            "16             0.776882                                   1    TP  \n",
            "17             0.750944                                   1    FP  \n",
            "18             0.000510                                   0    TN  \n",
            "19             0.084071                                   0    FN  \n",
            "\n",
            "\n",
            "Count of TP 219\n",
            "Count of FP 100\n",
            "Count of FN 142\n",
            "Count of TN 269\n",
            "precision 0.6865203761755486\n",
            "recall 0.6066481994459834\n",
            "F1 score is:  0.6441176470588237\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HAND CALCULATE F1 FOR LABSE BASELINE HYPERPARAMETERS FOR HINDI"
      ],
      "metadata": {
        "id": "umC470a9fTMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hand check F1 score for Hindi Lab Model\n",
        "\n",
        "\n",
        "# Load the Hindi predictions\n",
        "test_df_hindi_lab_base_predicted = pd.read_csv('/content/gdrive/My Drive/266_datasets/standard_hindi_data/hindi_lab_predictions.csv',\n",
        "                       sep = ',', \n",
        "                       dtype={'predicted_test_vals':float}\n",
        "                       )\n",
        "\n",
        "\n",
        "# Turn predictions into 0 and 1\n",
        "test_df_hindi_lab_base_predicted['Actual Predictions Hindi Lab Base'] = np.where(test_df_hindi_lab_base_predicted['predicted_test_vals'] < 0.5, 0,  1)\n",
        "\n",
        "# Drop unnamed columns\n",
        "test_df_hindi_lab_base_predicted = test_df_hindi_lab_base_predicted.drop(columns=['Unnamed: 0'], axis=1)\n",
        "\n",
        "\n",
        "# Concatenate the correct test data values with predicted data values\n",
        "test_df_hindi_lab_base_concat = pd.concat([test_df_hindi, test_df_hindi_lab_base_predicted], axis=1)\n",
        "\n",
        "\n",
        "# Find TP\n",
        "test_df_hindi_lab_base_concat['TP_FP'] = np.where((test_df_hindi_lab_base_concat['Actual Predictions Hindi Lab Base'] == test_df_hindi_lab_base_concat['label']) &  (test_df_hindi_lab_base_concat['label'] ==1), 'TP',  'unknown')\n",
        "\n",
        "\n",
        "# Find FP\n",
        "test_df_hindi_lab_base_concat['TP_FP'] = np.where((test_df_hindi_lab_base_concat['Actual Predictions Hindi Lab Base'] == 1) &  (test_df_hindi_lab_base_concat['label'] ==0), 'FP', test_df_hindi_lab_base_concat['TP_FP'])\n",
        "\n",
        "\n",
        "# Find FN\n",
        "test_df_hindi_lab_base_concat['TP_FP'] = np.where((test_df_hindi_lab_base_concat['Actual Predictions Hindi Lab Base'] == 0) &  (test_df_hindi_lab_base_concat['label'] ==1), 'FN', test_df_hindi_lab_base_concat['TP_FP'])\n",
        "\n",
        "\n",
        "# Find TN\n",
        "test_df_hindi_lab_base_concat['TP_FP'] = np.where((test_df_hindi_lab_base_concat['Actual Predictions Hindi Lab Base'] == 0) &  (test_df_hindi_lab_base_concat['label'] ==0), 'TN', test_df_hindi_lab_base_concat['TP_FP'])\n",
        "\n",
        "print(test_df_hindi_lab_base_concat.head(20))\n",
        "\n",
        "\n",
        "# Count TP\n",
        "count_tp_hindi_lab_base = test_df_hindi_lab_base_concat['TP_FP'].value_counts()['TP']\n",
        "print(\"\\n\\nCount of TP\",count_tp_hindi_lab_base)\n",
        "\n",
        "\n",
        "# Count FP\n",
        "count_fp_hindi_lab_base = test_df_hindi_lab_base_concat['TP_FP'].value_counts()['FP']\n",
        "print(\"Count of FP\", count_fp_hindi_lab_base)\n",
        "\n",
        "# Count FN\n",
        "count_fn_hindi_lab_base = test_df_hindi_lab_base_concat['TP_FP'].value_counts()['FN']\n",
        "print(\"Count of FN\",count_fn_hindi_lab_base)\n",
        "\n",
        "# Count TN\n",
        "count_tn_hindi_lab_base = test_df_hindi_lab_base_concat['TP_FP'].value_counts()['TN']\n",
        "print(\"Count of TN\",count_tn_hindi_lab_base)\n",
        "\n",
        "# Calculate Precision and Recall and F1 Score\n",
        "precision_hindi_lab_base = count_tp_hindi_lab_base / (count_tp_hindi_lab_base + count_fp_hindi_lab_base)\n",
        "print(\"precision\", precision_hindi_lab_base)\n",
        "\n",
        "recall_hindi_lab_base = count_tp_hindi_lab_base / (count_tp_hindi_lab_base  + count_fn_hindi_lab_base)\n",
        "\n",
        "print(\"recall\", recall_hindi_lab_base)\n",
        "f1_score_hindi_lab_base = (2 * precision_hindi_lab_base * recall_hindi_lab_base) / (precision_hindi_lab_base  + recall_hindi_lab_base)\n",
        "\n",
        "\n",
        "print(\"\\n\\nF1 score for Hindi Lab is: \", f1_score_hindi_lab_base )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHjxjW6efSg1",
        "outputId": "ec7c0b3f-8fb7-4a99-e4de-c394dadeba90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             comments  label  \\\n",
            "0   @zishanAliRJD @iAnantSingh_ *ओसामा साहब ने सिर...      0   \n",
            "1   @China_Amb_India @narendramodi I am shocked th...      0   \n",
            "2   कल से 18 से ऊपर वालो को हवा की वैक्सीन लगेगी.....      0   \n",
            "3   इधर की बात उधर करने में \\nआज भी जीमेल  से आगे ...      0   \n",
            "4   ☯️ मोदी जी ➡️ सोंगंद मुझे इस मिट्टी की मै देश ...      1   \n",
            "5   कबीर परमेश्वर जी ने अध्यात्म का विधान बताया है...      0   \n",
            "6   1990 रिपीट हो चुका है,1 लाख से ज्यादा हिन्दू ल...      0   \n",
            "7   \"MODI MEANS TERROR\"  #नरेंद्र_मोदी_ग्लोबल_पप्प...      1   \n",
            "8                      @Asrathour_143 और हिंदी में 😜😜      0   \n",
            "9   केंद्र में होते हुए भी, यदि आप कुछ नहीं कर पा ...      0   \n",
            "10  पप्पू लाल  इस लिए हाथ दिखा रहा है वह जानता है ...      1   \n",
            "11  @chitraaum बहन आपसे उम्मित हे आप इस विषय पर सं...      0   \n",
            "12  RT @MdNadim_BGS: ₹21000Cr की लागत से बनी भारत ...      0   \n",
            "13  It’s Time To Come Together To Save Farmers &am...      0   \n",
            "14  @anandkalra69 @Vandana__Indian हिन्दू इतने हरा...      1   \n",
            "15  @Naseeruddin_sah लालू की रिहाई की कीमत शहाबुद्...      0   \n",
            "16    इस की सकल देख कर इस को कोई रखेल भी नही बनायेगा।      1   \n",
            "17  इनको मांही खाना है, तो ये ससुरे सुअर, सुअर का ...      1   \n",
            "18  Adv Badshah Sadakat भाई का RJD से इस्तीफा!!! ह...      0   \n",
            "19  @TheOfficialSBI भाड़ मे जाओ एसबीआई वालों मैं त...      0   \n",
            "\n",
            "    predicted_test_vals  Actual Predictions Hindi Lab Base TP_FP  \n",
            "0              0.005121                                  0    TN  \n",
            "1              0.831543                                  1    FP  \n",
            "2              0.001159                                  0    TN  \n",
            "3              0.995518                                  1    FP  \n",
            "4              0.079734                                  0    FN  \n",
            "5              0.343505                                  0    TN  \n",
            "6              0.985221                                  1    FP  \n",
            "7              0.838533                                  1    TP  \n",
            "8              0.018075                                  0    TN  \n",
            "9              0.000543                                  0    TN  \n",
            "10             0.976520                                  1    TP  \n",
            "11             0.992665                                  1    FP  \n",
            "12             0.304514                                  0    TN  \n",
            "13             0.723122                                  1    FP  \n",
            "14             0.972517                                  1    TP  \n",
            "15             0.032652                                  0    TN  \n",
            "16             0.000869                                  0    FN  \n",
            "17             0.983988                                  1    TP  \n",
            "18             0.069258                                  0    TN  \n",
            "19             0.012836                                  0    TN  \n",
            "\n",
            "\n",
            "Count of TP 189\n",
            "Count of FP 63\n",
            "Count of FN 86\n",
            "Count of TN 192\n",
            "precision 0.75\n",
            "recall 0.6872727272727273\n",
            "\n",
            "\n",
            "F1 score for Hindi Lab is:  0.7172675521821631\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final concat of all results Hindi \n",
        "\n",
        "test_df_hindi_confusion = pd.concat([test_df_hindi, test_df_hindi_lab_base_concat, test_df_hindi_lab_concat, test_df_hindi_lab_cnn_concat], axis=1)"
      ],
      "metadata": {
        "id": "O08VrnMlnhMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_df_hindi_confusion.head())\n",
        "test_df_hindi_confusion.to_csv('/content/gdrive/My Drive/266_datasets/standard_hindi_data/hindi_confusion.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VhGm6zVoK_Z",
        "outputId": "fa20c938-636a-45fa-e89b-dde20fc21b1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            comments  label  \\\n",
            "0  @zishanAliRJD @iAnantSingh_ *ओसामा साहब ने सिर...      0   \n",
            "1  @China_Amb_India @narendramodi I am shocked th...      0   \n",
            "2  कल से 18 से ऊपर वालो को हवा की वैक्सीन लगेगी.....      0   \n",
            "3  इधर की बात उधर करने में \\nआज भी जीमेल  से आगे ...      0   \n",
            "4  ☯️ मोदी जी ➡️ सोंगंद मुझे इस मिट्टी की मै देश ...      1   \n",
            "\n",
            "                                            comments  label  \\\n",
            "0  @zishanAliRJD @iAnantSingh_ *ओसामा साहब ने सिर...      0   \n",
            "1  @China_Amb_India @narendramodi I am shocked th...      0   \n",
            "2  कल से 18 से ऊपर वालो को हवा की वैक्सीन लगेगी.....      0   \n",
            "3  इधर की बात उधर करने में \\nआज भी जीमेल  से आगे ...      0   \n",
            "4  ☯️ मोदी जी ➡️ सोंगंद मुझे इस मिट्टी की मै देश ...      1   \n",
            "\n",
            "   predicted_test_vals  Actual Predictions Hindi Lab Base TP_FP  \\\n",
            "0             0.005121                                  0    TN   \n",
            "1             0.831543                                  1    FP   \n",
            "2             0.001159                                  0    TN   \n",
            "3             0.995518                                  1    FP   \n",
            "4             0.079734                                  0    FN   \n",
            "\n",
            "                                            comments  label  \\\n",
            "0  @zishanAliRJD @iAnantSingh_ *ओसामा साहब ने सिर...      0   \n",
            "1  @China_Amb_India @narendramodi I am shocked th...      0   \n",
            "2  कल से 18 से ऊपर वालो को हवा की वैक्सीन लगेगी.....      0   \n",
            "3  इधर की बात उधर करने में \\nआज भी जीमेल  से आगे ...      0   \n",
            "4  ☯️ मोदी जी ➡️ सोंगंद मुझे इस मिट्टी की मै देश ...      1   \n",
            "\n",
            "   predicted_test_vals  Actual Predictions Hindi Lab TP_FP  \\\n",
            "0             0.473302                             0    TN   \n",
            "1             0.698759                             1    FP   \n",
            "2             0.054247                             0    TN   \n",
            "3             0.762390                             1    FP   \n",
            "4             0.463459                             0    FN   \n",
            "\n",
            "                                            comments  label  \\\n",
            "0  @zishanAliRJD @iAnantSingh_ *ओसामा साहब ने सिर...      0   \n",
            "1  @China_Amb_India @narendramodi I am shocked th...      0   \n",
            "2  कल से 18 से ऊपर वालो को हवा की वैक्सीन लगेगी.....      0   \n",
            "3  इधर की बात उधर करने में \\nआज भी जीमेल  से आगे ...      0   \n",
            "4  ☯️ मोदी जी ➡️ सोंगंद मुझे इस मिट्टी की मै देश ...      1   \n",
            "\n",
            "   predicted_test_vals  Actual Predictions Hindi Lab CNN TP_FP  \n",
            "0             0.878002                                 1    FP  \n",
            "1             0.518621                                 1    FP  \n",
            "2             0.113499                                 0    TN  \n",
            "3             0.424176                                 0    TN  \n",
            "4             0.658142                                 1    TP  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final concat of all results Korean\n",
        "\n",
        "test_df_korean_confusion = pd.concat([test_df_korean, test_df_korean_lab_base_concat, test_df_korean_lab_concat, test_df_korean_lab_cnn_concat], axis=1)"
      ],
      "metadata": {
        "id": "IIS0Ul73qqb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_df_korean_confusion.head())\n",
        "test_df_korean_confusion.to_csv('/content/gdrive/My Drive/266_datasets/standard_korean_data/korean_confusion.csv')"
      ],
      "metadata": {
        "id": "5RSFC4TPq1Bg",
        "outputId": "3007880a-5c58-42cd-8d09-3387acbfa673",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            comments  label  \\\n",
            "0  팀으로 데뷔한거면 개인활동 했어도 N빵 해야지... 그게 팀을 위해서도 맞는거고~~...      0   \n",
            "1                  왕지혜 34살이지만 외모는 인정한다.여자라면 이정도는 되야지      0   \n",
            "2                           여자들이 80프로잉 악플 남자는 여자욕 안해      1   \n",
            "3                     설현이 떨고 있다... 아니겠지 아닐거야 그것만은 안돼      0   \n",
            "4                 다된 기생충 잔치에재 뿌린 방가방송과 안현모 다신 나서지 말자      1   \n",
            "\n",
            "                                            comments  label  \\\n",
            "0  팀으로 데뷔한거면 개인활동 했어도 N빵 해야지... 그게 팀을 위해서도 맞는거고~~...      0   \n",
            "1                  왕지혜 34살이지만 외모는 인정한다.여자라면 이정도는 되야지      0   \n",
            "2                           여자들이 80프로잉 악플 남자는 여자욕 안해      1   \n",
            "3                     설현이 떨고 있다... 아니겠지 아닐거야 그것만은 안돼      0   \n",
            "4                 다된 기생충 잔치에재 뿌린 방가방송과 안현모 다신 나서지 말자      1   \n",
            "\n",
            "   predicted_test_vals  Actual Predictions Korean Lab Base TP_FP  \\\n",
            "0             0.073717                                   0    TN   \n",
            "1             0.971796                                   1    FP   \n",
            "2             0.997700                                   1    TP   \n",
            "3             0.017122                                   0    TN   \n",
            "4             0.726807                                   1    TP   \n",
            "\n",
            "                                            comments  label  \\\n",
            "0  팀으로 데뷔한거면 개인활동 했어도 N빵 해야지... 그게 팀을 위해서도 맞는거고~~...      0   \n",
            "1                  왕지혜 34살이지만 외모는 인정한다.여자라면 이정도는 되야지      0   \n",
            "2                           여자들이 80프로잉 악플 남자는 여자욕 안해      1   \n",
            "3                     설현이 떨고 있다... 아니겠지 아닐거야 그것만은 안돼      0   \n",
            "4                 다된 기생충 잔치에재 뿌린 방가방송과 안현모 다신 나서지 말자      1   \n",
            "\n",
            "   predicted_test_vals  Actual Predictions Korean Lab TP_FP  \\\n",
            "0             0.480018                              0    TN   \n",
            "1             0.823669                              1    FP   \n",
            "2             0.985263                              1    TP   \n",
            "3             0.611882                              1    FP   \n",
            "4             0.980416                              1    TP   \n",
            "\n",
            "                                            comments  label  \\\n",
            "0  팀으로 데뷔한거면 개인활동 했어도 N빵 해야지... 그게 팀을 위해서도 맞는거고~~...      0   \n",
            "1                  왕지혜 34살이지만 외모는 인정한다.여자라면 이정도는 되야지      0   \n",
            "2                           여자들이 80프로잉 악플 남자는 여자욕 안해      1   \n",
            "3                     설현이 떨고 있다... 아니겠지 아닐거야 그것만은 안돼      0   \n",
            "4                 다된 기생충 잔치에재 뿌린 방가방송과 안현모 다신 나서지 말자      1   \n",
            "\n",
            "   predicted_test_vals  Actual Predictions Korean Lab CNN TP_FP  \n",
            "0             0.344278                                  0    TN  \n",
            "1             0.602851                                  1    FP  \n",
            "2             0.939980                                  1    TP  \n",
            "3             0.414587                                  0    TN  \n",
            "4             0.777714                                  1    TP  \n"
          ]
        }
      ]
    }
  ]
}